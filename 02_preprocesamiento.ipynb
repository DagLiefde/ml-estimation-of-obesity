{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#  Preprocesamiento de Datos\n",
        "## Clasificación de Niveles de Obesidad - Regresión Ordinal\n",
        "\n",
        "Este notebook realiza el preprocesamiento completo de los datos antes de entrenar los modelos.\n",
        "\n",
        "**Objetivos del preprocesamiento:**\n",
        "1. Codificar variables categóricas (convertir texto a números)\n",
        "2. Normalizar/estandarizar variables numéricas\n",
        "3. Dividir datos en Train (70%) y Test (30%) con estratificación\n",
        "4. Guardar datos preprocesados y transformadores\n",
        "\n",
        "**Distribución**: 70% Train (con validación cruzada) / 30% Test\n",
        "\n",
        "---\n",
        "\n",
        "## ¿Por qué es necesario el preprocesamiento?\n",
        "\n",
        "Los algoritmos de Machine Learning requieren que los datos estén en un formato específico:\n",
        "\n",
        "- **Variables categóricas**: Deben convertirse a números (encoding)\n",
        "- **Escalas diferentes**: Variables con rangos muy diferentes (ej: Age 0-100 vs Height 1.5-2.0) pueden sesgar el modelo\n",
        "- **División de datos**: Necesitamos separar datos para entrenar y evaluar\n",
        "\n",
        "Sin preprocesamiento adecuado, los modelos pueden tener mal rendimiento o incluso fallar.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Importación de Librerías\n",
        "\n",
        "### ¿Por qué estas librerías?\n",
        "\n",
        "- **pandas**: Manipulación de datos estructurados (DataFrames)\n",
        "- **numpy**: Operaciones numéricas y matemáticas\n",
        "- **sklearn**: Proporciona herramientas de preprocesamiento (StandardScaler, LabelEncoder, etc.) y división de datos\n",
        "- **pickle**: Guardar objetos Python (transformadores) para uso futuro\n",
        "- **os**: Crear directorios si no existen\n",
        "\n",
        "### ¿Por qué guardar los transformadores?\n",
        "\n",
        "Es crucial guardar los transformadores (scaler, encoders) porque:\n",
        "- Cuando tengamos nuevos datos, debemos aplicar las **mismas transformaciones**\n",
        "- El scaler debe usar los mismos parámetros (media y desviación estándar) aprendidos del train\n",
        "- Sin esto, los nuevos datos estarían en una escala diferente y las predicciones serían incorrectas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Librerías importadas correctamente\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "import pickle\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\" Librerías importadas correctamente\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Carga del Dataset\n",
        "\n",
        "### ¿Qué hace este paso?\n",
        "\n",
        "Carga el archivo CSV original y verifica que:\n",
        "- El archivo se carga correctamente\n",
        "- No hay valores faltantes\n",
        "- La estructura es la esperada\n",
        "\n",
        "### ¿Por qué verificar valores faltantes?\n",
        "\n",
        "Los valores faltantes pueden causar problemas:\n",
        "- Algunos algoritmos no pueden manejar NaN directamente\n",
        "- Necesitamos decidir cómo tratarlos (eliminar, imputar, etc.)\n",
        "- En este caso, verificamos que no haya valores faltantes antes de continuar\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Dataset cargado exitosamente\n",
            "  - Forma del dataset: (2111, 17)\n",
            "  - Columnas: ['Gender', 'Age', 'Height', 'Weight', 'family_history_with_overweight', 'FAVC', 'FCVC', 'NCP', 'CAEC', 'SMOKE', 'CH2O', 'SCC', 'FAF', 'TUE', 'CALC', 'MTRANS', 'NObeyesdad']\n",
            "\n",
            "Primeras 3 filas:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Gender</th>\n",
              "      <th>Age</th>\n",
              "      <th>Height</th>\n",
              "      <th>Weight</th>\n",
              "      <th>family_history_with_overweight</th>\n",
              "      <th>FAVC</th>\n",
              "      <th>FCVC</th>\n",
              "      <th>NCP</th>\n",
              "      <th>CAEC</th>\n",
              "      <th>SMOKE</th>\n",
              "      <th>CH2O</th>\n",
              "      <th>SCC</th>\n",
              "      <th>FAF</th>\n",
              "      <th>TUE</th>\n",
              "      <th>CALC</th>\n",
              "      <th>MTRANS</th>\n",
              "      <th>NObeyesdad</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Female</td>\n",
              "      <td>21.0</td>\n",
              "      <td>1.62</td>\n",
              "      <td>64.0</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>Sometimes</td>\n",
              "      <td>no</td>\n",
              "      <td>2.0</td>\n",
              "      <td>no</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>no</td>\n",
              "      <td>Public_Transportation</td>\n",
              "      <td>Normal_Weight</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Female</td>\n",
              "      <td>21.0</td>\n",
              "      <td>1.52</td>\n",
              "      <td>56.0</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>Sometimes</td>\n",
              "      <td>yes</td>\n",
              "      <td>3.0</td>\n",
              "      <td>yes</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Sometimes</td>\n",
              "      <td>Public_Transportation</td>\n",
              "      <td>Normal_Weight</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Male</td>\n",
              "      <td>23.0</td>\n",
              "      <td>1.80</td>\n",
              "      <td>77.0</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>Sometimes</td>\n",
              "      <td>no</td>\n",
              "      <td>2.0</td>\n",
              "      <td>no</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Frequently</td>\n",
              "      <td>Public_Transportation</td>\n",
              "      <td>Normal_Weight</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Gender   Age  Height  Weight family_history_with_overweight FAVC  FCVC  \\\n",
              "0  Female  21.0    1.62    64.0                            yes   no   2.0   \n",
              "1  Female  21.0    1.52    56.0                            yes   no   3.0   \n",
              "2    Male  23.0    1.80    77.0                            yes   no   2.0   \n",
              "\n",
              "   NCP       CAEC SMOKE  CH2O  SCC  FAF  TUE        CALC  \\\n",
              "0  3.0  Sometimes    no   2.0   no  0.0  1.0          no   \n",
              "1  3.0  Sometimes   yes   3.0  yes  3.0  0.0   Sometimes   \n",
              "2  3.0  Sometimes    no   2.0   no  2.0  1.0  Frequently   \n",
              "\n",
              "                  MTRANS     NObeyesdad  \n",
              "0  Public_Transportation  Normal_Weight  \n",
              "1  Public_Transportation  Normal_Weight  \n",
              "2  Public_Transportation  Normal_Weight  "
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Cargar el dataset\n",
        "df = pd.read_csv('ObesityDataSet_raw_and_data_sinthetic.csv')\n",
        "\n",
        "print(f\" Dataset cargado exitosamente\")\n",
        "print(f\"  - Forma del dataset: {df.shape}\")\n",
        "print(f\"  - Columnas: {list(df.columns)}\")\n",
        "print(f\"\\nPrimeras 3 filas:\")\n",
        "df.head(3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Valores faltantes por columna:\n",
            "   No hay valores faltantes\n"
          ]
        }
      ],
      "source": [
        "# Verificar valores faltantes\n",
        "print(f\"Valores faltantes por columna:\")\n",
        "missing = df.isnull().sum()\n",
        "print(missing[missing > 0] if missing.sum() > 0 else \"   No hay valores faltantes\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Creación de Variable IMC (Índice de Masa Corporal)\n",
        "\n",
        "### ¿Qué hace este paso?\n",
        "\n",
        "Calcula el IMC (BMI) a partir de Weight y Height, y elimina las variables originales.\n",
        "\n",
        "### ¿Por qué usar IMC en lugar de Weight y Height?\n",
        "\n",
        "**IMC (Índice de Masa Corporal)** = Peso (kg) / Altura (m)²\n",
        "\n",
        "**Ventajas**:\n",
        "1. **Relevancia clínica**: El IMC es la métrica estándar para clasificar obesidad\n",
        "2. **Reducción de dimensionalidad**: 2 variables (Weight, Height) → 1 variable (BMI)\n",
        "3. **Reduce multicolinealidad**: Weight y Height están altamente correlacionadas\n",
        "4. **Interpretabilidad**: El IMC es más interpretable y significativo\n",
        "5. **Normalización natural**: El IMC ya normaliza por altura\n",
        "\n",
        "**Las clases de obesidad están basadas en rangos de IMC**:\n",
        "- Insufficient_Weight: IMC < 18.5\n",
        "- Normal_Weight: 18.5 ≤ IMC < 25\n",
        "- Overweight_Level_I: 25 ≤ IMC < 27\n",
        "- Overweight_Level_II: 27 ≤ IMC < 30\n",
        "- Obesity_Type_I: 30 ≤ IMC < 35\n",
        "- Obesity_Type_II: 35 ≤ IMC < 40\n",
        "- Obesity_Type_III: IMC ≥ 40\n",
        "\n",
        "### ¿Por qué eliminar Weight y Height?\n",
        "\n",
        "- El IMC captura la información relevante de ambas variables\n",
        "- Evita multicolinealidad (correlación alta entre variables)\n",
        "- Simplifica el modelo sin perder información relevante para este problema\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Calculando IMC (Índice de Masa Corporal)...\n",
            " IMC calculado\n",
            "  - Rango de IMC: 13.00 - 50.81\n",
            "  - Media de IMC: 29.70\n",
            "\n",
            "Ejemplos de cálculo:\n",
            "   Height  Weight        BMI           NObeyesdad\n",
            "0    1.62    64.0  24.386526        Normal_Weight\n",
            "1    1.52    56.0  24.238227        Normal_Weight\n",
            "2    1.80    77.0  23.765432        Normal_Weight\n",
            "3    1.80    87.0  26.851852   Overweight_Level_I\n",
            "4    1.78    89.8  28.342381  Overweight_Level_II\n",
            "\n",
            " Eliminando variables Weight y Height...\n",
            "   Variables eliminadas\n",
            "  - Columnas restantes: 16\n",
            "\n",
            " Variable objetivo separada: 'NObeyesdad'\n",
            "  - Forma de y: (2111,)\n",
            "  - Forma de X: (2111, 15)\n",
            "\n",
            " ANÁLISIS DE TIPOS DE VARIABLES:\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Variables Numéricas (7):\n",
            "  - Age: min=14.00, max=61.00, media=24.31\n",
            "  - FCVC: min=1.00, max=3.00, media=2.42\n",
            "  - NCP: min=1.00, max=4.00, media=2.69\n",
            "  - CH2O: min=1.00, max=3.00, media=2.01\n",
            "  - FAF: min=0.00, max=3.00, media=1.01\n",
            "  - TUE: min=0.00, max=2.00, media=0.66\n",
            "  - BMI: min=13.00, max=50.81, media=29.70\n",
            "\n",
            "Variables Categóricas (8):\n",
            "  - Gender: 2 valores únicos -> ['Female', 'Male']...\n",
            "  - family_history_with_overweight: 2 valores únicos -> ['yes', 'no']...\n",
            "  - FAVC: 2 valores únicos -> ['no', 'yes']...\n",
            "  - CAEC: 4 valores únicos -> ['Sometimes', 'Frequently', 'Always', 'no']...\n",
            "  - SMOKE: 2 valores únicos -> ['no', 'yes']...\n",
            "  - SCC: 2 valores únicos -> ['no', 'yes']...\n",
            "  - CALC: 4 valores únicos -> ['no', 'Sometimes', 'Frequently', 'Always']...\n",
            "  - MTRANS: 5 valores únicos -> ['Public_Transportation', 'Walking', 'Automobile', 'Motorbike', 'Bike']...\n"
          ]
        }
      ],
      "source": [
        "# Calcular IMC (BMI) = Weight (kg) / Height (m)²\n",
        "print(\" Calculando IMC (Índice de Masa Corporal)...\")\n",
        "df['BMI'] = df['Weight'] / (df['Height'] ** 2)\n",
        "\n",
        "print(f\" IMC calculado\")\n",
        "print(f\"  - Rango de IMC: {df['BMI'].min():.2f} - {df['BMI'].max():.2f}\")\n",
        "print(f\"  - Media de IMC: {df['BMI'].mean():.2f}\")\n",
        "\n",
        "# Mostrar algunos ejemplos antes de eliminar\n",
        "print(f\"\\nEjemplos de cálculo:\")\n",
        "print(df[['Height', 'Weight', 'BMI', 'NObeyesdad']].head())\n",
        "\n",
        "# Eliminar Weight y Height (ya no las necesitamos)\n",
        "print(f\"\\n Eliminando variables Weight y Height...\")\n",
        "df = df.drop(columns=['Weight', 'Height'])\n",
        "print(f\"   Variables eliminadas\")\n",
        "print(f\"  - Columnas restantes: {len(df.columns)}\")\n",
        "\n",
        "# Variable objetivo\n",
        "target_column = 'NObeyesdad'\n",
        "y = df[target_column].copy()\n",
        "\n",
        "# Características (todas las columnas excepto la objetivo)\n",
        "X = df.drop(columns=[target_column])\n",
        "\n",
        "print(f\"\\n Variable objetivo separada: '{target_column}'\")\n",
        "print(f\"  - Forma de y: {y.shape}\")\n",
        "print(f\"  - Forma de X: {X.shape}\")\n",
        "\n",
        "# Identificar tipos de variables\n",
        "print(f\"\\n ANÁLISIS DE TIPOS DE VARIABLES:\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Variables numéricas (ya son números)\n",
        "numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
        "print(f\"\\nVariables Numéricas ({len(numeric_cols)}):\")\n",
        "for col in numeric_cols:\n",
        "    print(f\"  - {col}: min={X[col].min():.2f}, max={X[col].max():.2f}, media={X[col].mean():.2f}\")\n",
        "\n",
        "# Variables categóricas (objetos/strings)\n",
        "categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
        "print(f\"\\nVariables Categóricas ({len(categorical_cols)}):\")\n",
        "for col in categorical_cols:\n",
        "    unique_vals = X[col].unique()\n",
        "    print(f\"  - {col}: {len(unique_vals)} valores únicos -> {list(unique_vals)[:5]}...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2111 entries, 0 to 2110\n",
            "Data columns (total 15 columns):\n",
            " #   Column                          Non-Null Count  Dtype  \n",
            "---  ------                          --------------  -----  \n",
            " 0   Gender                          2111 non-null   object \n",
            " 1   Age                             2111 non-null   float64\n",
            " 2   family_history_with_overweight  2111 non-null   object \n",
            " 3   FAVC                            2111 non-null   object \n",
            " 4   FCVC                            2111 non-null   float64\n",
            " 5   NCP                             2111 non-null   float64\n",
            " 6   CAEC                            2111 non-null   object \n",
            " 7   SMOKE                           2111 non-null   object \n",
            " 8   CH2O                            2111 non-null   float64\n",
            " 9   SCC                             2111 non-null   object \n",
            " 10  FAF                             2111 non-null   float64\n",
            " 11  TUE                             2111 non-null   float64\n",
            " 12  CALC                            2111 non-null   object \n",
            " 13  MTRANS                          2111 non-null   object \n",
            " 14  BMI                             2111 non-null   float64\n",
            "dtypes: float64(7), object(8)\n",
            "memory usage: 247.5+ KB\n"
          ]
        }
      ],
      "source": [
        "X.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Encoding de Variables Categóricas\n",
        "\n",
        "### ¿Qué hace este paso?\n",
        "\n",
        "Convierte variables categóricas (texto) a números. Los algoritmos de ML solo pueden trabajar con números.\n",
        "\n",
        "### ¿Por qué necesitamos encoding?\n",
        "\n",
        "Los modelos matemáticos no entienden texto. Necesitamos convertir:\n",
        "- \"Male\" / \"Female\" → números\n",
        "- \"yes\" / \"no\" → números\n",
        "- \"Public_Transportation\" / \"Walking\" / etc. → números\n",
        "\n",
        "### Dos Estrategias de Encoding\n",
        "\n",
        "#### 1. Label Encoding\n",
        "- Convierte cada categoría a un número (0, 1, 2, ...)\n",
        "- Ejemplo: \"no\" → 0, \"yes\" → 1\n",
        "- **Usamos para variables binarias** (solo 2 valores)\n",
        "\n",
        "#### 2. One-Hot Encoding\n",
        "- Crea una columna binaria por cada categoría\n",
        "- Ejemplo: \"Public_Transportation\" → [1, 0, 0, 0, 0]\n",
        "- **Usamos para variables con múltiples categorías sin orden**\n",
        "- Evita que el modelo asuma orden donde no lo hay\n",
        "\n",
        "### ¿Por qué diferentes estrategias?\n",
        "\n",
        "- **Label Encoding para binarias**: Más eficiente, no crea columnas extra\n",
        "- **One-Hot para multi-categoría**: Evita que el modelo piense que hay orden (ej: \"Walking\" no es \"menor\" que \"Automobile\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Variables Binarias (Label Encoding): ['Gender', 'family_history_with_overweight', 'FAVC', 'SMOKE', 'SCC']\n",
            "Variables Multi-Categoría (One-Hot Encoding): ['CAEC', 'CALC', 'MTRANS']\n"
          ]
        }
      ],
      "source": [
        "# Crear una copia para trabajar\n",
        "X_encoded = X.copy()\n",
        "\n",
        "# Identificar variables binarias (solo 2 valores)\n",
        "binary_cols = []\n",
        "multi_cat_cols = []\n",
        "\n",
        "for col in categorical_cols:\n",
        "    n_unique = X[col].nunique()\n",
        "    if n_unique == 2:\n",
        "        binary_cols.append(col)\n",
        "    else:\n",
        "        multi_cat_cols.append(col)\n",
        "\n",
        "print(f\"Variables Binarias (Label Encoding): {binary_cols}\")\n",
        "print(f\"Variables Multi-Categoría (One-Hot Encoding): {multi_cat_cols}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Aplicando Label Encoding a variables binarias...\n",
            "   Gender: {'Female': 0, 'Male': 1}\n",
            "   family_history_with_overweight: {'no': 0, 'yes': 1}\n",
            "   FAVC: {'no': 0, 'yes': 1}\n",
            "   SMOKE: {'no': 0, 'yes': 1}\n",
            "   SCC: {'no': 0, 'yes': 1}\n"
          ]
        }
      ],
      "source": [
        "# 4.1: Label Encoding para variables binarias\n",
        "print(f\"\\n Aplicando Label Encoding a variables binarias...\")\n",
        "label_encoders = {}\n",
        "\n",
        "for col in binary_cols:\n",
        "    le = LabelEncoder()\n",
        "    X_encoded[col] = le.fit_transform(X[col])\n",
        "    label_encoders[col] = le\n",
        "    print(f\"   {col}: {dict(zip(le.classes_, le.transform(le.classes_)))}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Aplicando One-Hot Encoding a variables multi-categoría...\n",
            "   Variables codificadas:\n",
            "    - Antes: 8 variables categóricas\n",
            "    - Después: 25 variables totales\n",
            "    - Nuevas columnas creadas: 13\n",
            "\n",
            "  Ejemplo de nuevas columnas (primeras 5):\n",
            "    - CAEC_Always\n",
            "    - CAEC_Frequently\n",
            "    - CAEC_Sometimes\n",
            "    - CAEC_no\n",
            "    - CALC_Always\n"
          ]
        }
      ],
      "source": [
        "# 4.2: One-Hot Encoding para variables multi-categoría\n",
        "print(f\"\\n Aplicando One-Hot Encoding a variables multi-categoría...\")\n",
        "\n",
        "# Usar pandas get_dummies (más simple que sklearn para este caso)\n",
        "X_encoded = pd.get_dummies(X_encoded, columns=multi_cat_cols, prefix=multi_cat_cols, drop_first=False)\n",
        "\n",
        "print(f\"   Variables codificadas:\")\n",
        "print(f\"    - Antes: {len(categorical_cols)} variables categóricas\")\n",
        "print(f\"    - Después: {X_encoded.shape[1]} variables totales\")\n",
        "print(f\"    - Nuevas columnas creadas: {X_encoded.shape[1] - len(numeric_cols) - len(binary_cols)}\")\n",
        "\n",
        "# Mostrar algunas columnas nuevas\n",
        "new_cols = [col for col in X_encoded.columns if any(mc in col for mc in multi_cat_cols)]\n",
        "print(f\"\\n  Ejemplo de nuevas columnas (primeras 5):\")\n",
        "for col in new_cols[:5]:\n",
        "    print(f\"    - {col}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Normalización/Estandarización de Variables Numéricas\n",
        "\n",
        "### ¿Qué hace este paso?\n",
        "\n",
        "Normaliza/estandariza las variables numéricas para que todas estén en la misma escala.\n",
        "\n",
        "### ¿Por qué es importante?\n",
        "\n",
        "**Problema**: Variables con rangos muy diferentes pueden dominar el modelo.\n",
        "\n",
        "**Ejemplo**:\n",
        "- Age: 0-100 (rango grande)\n",
        "- Height: 1.5-2.0 (rango pequeño)\n",
        "\n",
        "Sin estandarización, Age podría tener más \"peso\" en el modelo simplemente por tener números más grandes, aunque Height podría ser igual de importante.\n",
        "\n",
        "### Tipos de Normalización\n",
        "\n",
        "#### Estandarización (Z-score normalization) - **USAMOS ESTA**\n",
        "- **Fórmula**: `z = (x - μ) / σ`\n",
        "- Convierte datos a: media = 0, desviación estándar = 1\n",
        "- Útil cuando los datos siguen distribución normal\n",
        "- **Ventaja**: Funciona bien con la mayoría de algoritmos\n",
        "\n",
        "#### Normalización (Min-Max)\n",
        "- **Fórmula**: `x_norm = (x - min) / (max - min)`\n",
        "- Convierte datos a rango [0, 1]\n",
        "- Útil cuando no conocemos la distribución\n",
        "\n",
        "### ¿Cuándo NO estandarizar?\n",
        "\n",
        "- **Árboles de decisión** (Random Forest, Gradient Boosting): No necesitan estandarización porque dividen por umbrales\n",
        "- **Naive Bayes**: Puede funcionar sin estandarización\n",
        "- **Pero**: SVM, k-NN, redes neuronales SÍ necesitan estandarización\n",
        "\n",
        "**Para este proyecto**: Estandarizamos porque usaremos varios tipos de modelos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Variables a estandarizar (12):\n",
            "  ['Age', 'FCVC', 'NCP', 'CH2O', 'FAF', 'TUE', 'BMI', 'Gender', 'family_history_with_overweight', 'FAVC', 'SMOKE', 'SCC']\n",
            "\n",
            " Scaler creado (se aplicará después de dividir train/test)\n",
            "  - Tipo: StandardScaler (Z-score normalization)\n",
            "  - Fórmula: z = (x - μ) / σ\n",
            "\n",
            " Estadísticas ANTES de estandarizar (primeras 3 variables):\n",
            "  Age:\n",
            "    - Media: 24.3126\n",
            "    - Desv. Est.: 6.3460\n",
            "    - Min: 14.0000\n",
            "    - Max: 61.0000\n",
            "  FCVC:\n",
            "    - Media: 2.4190\n",
            "    - Desv. Est.: 0.5339\n",
            "    - Min: 1.0000\n",
            "    - Max: 3.0000\n",
            "  NCP:\n",
            "    - Media: 2.6856\n",
            "    - Desv. Est.: 0.7780\n",
            "    - Min: 1.0000\n",
            "    - Max: 4.0000\n"
          ]
        }
      ],
      "source": [
        "# Identificar columnas numéricas (después del encoding)\n",
        "# Las columnas numéricas originales + las binarias codificadas\n",
        "numeric_cols_to_scale = numeric_cols + binary_cols\n",
        "\n",
        "print(f\"Variables a estandarizar ({len(numeric_cols_to_scale)}):\")\n",
        "print(f\"  {numeric_cols_to_scale}\")\n",
        "\n",
        "# Crear el scaler (pero NO aplicarlo aún - lo haremos después de dividir)\n",
        "# Esto es importante: NO debemos estandarizar antes de dividir train/test\n",
        "# porque podríamos \"filtrar\" información del test al train\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "print(f\"\\n Scaler creado (se aplicará después de dividir train/test)\")\n",
        "print(f\"  - Tipo: StandardScaler (Z-score normalization)\")\n",
        "print(f\"  - Fórmula: z = (x - μ) / σ\")\n",
        "\n",
        "# Mostrar estadísticas antes de estandarizar\n",
        "print(f\"\\n Estadísticas ANTES de estandarizar (primeras 3 variables):\")\n",
        "for col in numeric_cols_to_scale[:3]:\n",
        "    print(f\"  {col}:\")\n",
        "    print(f\"    - Media: {X_encoded[col].mean():.4f}\")\n",
        "    print(f\"    - Desv. Est.: {X_encoded[col].std():.4f}\")\n",
        "    print(f\"    - Min: {X_encoded[col].min():.4f}\")\n",
        "    print(f\"    - Max: {X_encoded[col].max():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. División de Datos (70-30) con Estratificación\n",
        "\n",
        "### ¿Qué hace este paso?\n",
        "\n",
        "Divide los datos en Train (70%) y Test (30%) manteniendo la proporción de clases en ambos conjuntos.\n",
        "\n",
        "### ¿Por qué dividir los datos?\n",
        "\n",
        "- **Train (70%)**: Usamos para entrenar y ajustar los modelos (con validación cruzada)\n",
        "- **Test (30%)**: Usamos SOLO al final para evaluar el modelo final\n",
        "- **Nunca** usamos test durante el entrenamiento (evita sobreajuste)\n",
        "\n",
        "### ¿Qué es la Estratificación?\n",
        "\n",
        "**Estratificación** = Mantener la proporción de clases en ambos conjuntos.\n",
        "\n",
        "**Ejemplo sin estratificación**:\n",
        "- Train: 80% Normal_Weight, 5% Obesity_Type_III\n",
        "- Test: 10% Normal_Weight, 30% Obesity_Type_III\n",
        "-  Problema: Los conjuntos no son representativos\n",
        "\n",
        "**Ejemplo con estratificación**:\n",
        "- Train: 13.6% Normal_Weight, 15.3% Obesity_Type_III\n",
        "- Test: 13.6% Normal_Weight, 15.3% Obesity_Type_III\n",
        "-  Ambos conjuntos tienen la misma distribución\n",
        "\n",
        "### ¿Por qué NO estandarizar antes de dividir?\n",
        "\n",
        "**IMPORTANTE**: NO estandarizamos antes de dividir porque:\n",
        "\n",
        "1. El scaler se ajusta SOLO con datos de train\n",
        "2. Luego se aplica a test (sin reajustar)\n",
        "3. Esto evita **\"data leakage\"** (filtrar información del test al train)\n",
        "\n",
        "Si estandarizamos antes de dividir:\n",
        "- El scaler \"ve\" todos los datos (train + test)\n",
        "- Esto filtra información del test al train\n",
        "- El modelo podría tener mejor rendimiento de lo que realmente tiene\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " División completada:\n",
            "  - Train: 1477 registros (70.0%)\n",
            "  - Test: 634 registros (30.0%)\n",
            "  - Características: 25\n"
          ]
        }
      ],
      "source": [
        "# División estratificada\n",
        "# random_state: Para reproducibilidad (mismos resultados cada vez)\n",
        "# stratify: Mantiene proporción de clases\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_encoded,\n",
        "    y,\n",
        "    test_size=0.30,  # 30% para test\n",
        "    random_state=42,  # Semilla para reproducibilidad\n",
        "    stratify=y  # Estratificación por clases\n",
        ")\n",
        "\n",
        "print(f\"\\n División completada:\")\n",
        "print(f\"  - Train: {X_train.shape[0]} registros ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
        "print(f\"  - Test: {X_test.shape[0]} registros ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
        "print(f\"  - Características: {X_train.shape[1]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Verificación de estratificación:\n",
            "\n",
            "Distribución en Train:\n",
            "  Insufficient_Weight      :  190 (12.86%)\n",
            "  Normal_Weight            :  201 (13.61%)\n",
            "  Obesity_Type_I           :  245 (16.59%)\n",
            "  Obesity_Type_II          :  208 (14.08%)\n",
            "  Obesity_Type_III         :  227 (15.37%)\n",
            "  Overweight_Level_I       :  203 (13.74%)\n",
            "  Overweight_Level_II      :  203 (13.74%)\n",
            "\n",
            "Distribución en Test:\n",
            "  Insufficient_Weight      :   82 (12.93%)\n",
            "  Normal_Weight            :   86 (13.56%)\n",
            "  Obesity_Type_I           :  106 (16.72%)\n",
            "  Obesity_Type_II          :   89 (14.04%)\n",
            "  Obesity_Type_III         :   97 (15.30%)\n",
            "  Overweight_Level_I       :   87 (13.72%)\n",
            "  Overweight_Level_II      :   87 (13.72%)\n",
            "\n",
            " Verificación: Las proporciones son similares en ambos conjuntos\n"
          ]
        }
      ],
      "source": [
        "# Verificar estratificación\n",
        "print(f\"\\n Verificación de estratificación:\")\n",
        "print(f\"\\nDistribución en Train:\")\n",
        "train_dist = y_train.value_counts().sort_index()\n",
        "train_pct = (y_train.value_counts(normalize=True) * 100).sort_index()\n",
        "for clase in train_dist.index:\n",
        "    print(f\"  {clase:25s}: {train_dist[clase]:4d} ({train_pct[clase]:5.2f}%)\")\n",
        "\n",
        "print(f\"\\nDistribución en Test:\")\n",
        "test_dist = y_test.value_counts().sort_index()\n",
        "test_pct = (y_test.value_counts(normalize=True) * 100).sort_index()\n",
        "for clase in test_dist.index:\n",
        "    print(f\"  {clase:25s}: {test_dist[clase]:4d} ({test_pct[clase]:5.2f}%)\")\n",
        "\n",
        "# Verificar que las proporciones son similares\n",
        "print(f\"\\n Verificación: Las proporciones son similares en ambos conjuntos\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Estandarización de Datos (DESPUÉS de Dividir)\n",
        "\n",
        "### ¿Qué hace este paso?\n",
        "\n",
        "Aplica la estandarización SOLO a los datos de train, ajusta el scaler con train, y luego aplica la misma transformación a test (sin reajustar).\n",
        "\n",
        "### ¿Por qué este orden es crítico?\n",
        "\n",
        "**Proceso correcto**:\n",
        "1. Dividir datos (train/test)\n",
        "2. Ajustar scaler con SOLO train → aprende media y desviación estándar de train\n",
        "3. Transformar train con el scaler ajustado\n",
        "4. Transformar test con el MISMO scaler (sin reajustar)\n",
        "\n",
        "**Si hiciéramos mal**:\n",
        "1. Estandarizar todo el dataset\n",
        "2. Dividir datos\n",
        "3.  El scaler \"vio\" datos de test → data leakage\n",
        "\n",
        "### ¿Por qué no reajustar el scaler con test?\n",
        "\n",
        "Porque en producción:\n",
        "- Nuevos datos llegan sin etiquetas\n",
        "- Debemos aplicar las mismas transformaciones aprendidas del train\n",
        "- Si reajustamos con test, estaríamos \"haciendo trampa\"\n",
        "\n",
        "### Resultado Esperado\n",
        "\n",
        "Después de estandarizar:\n",
        "- **Media ≈ 0**: Los datos están centrados\n",
        "- **Desviación estándar ≈ 1**: Los datos están escalados\n",
        "- Esto facilita el entrenamiento de modelos sensibles a la escala\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Ajustando scaler con datos de train...\n",
            " Estandarización aplicada\n",
            "\n",
            " Estadísticas DESPUÉS de estandarizar (primeras 3 variables):\n",
            "  Age:\n",
            "    - Media: 0.000000 (debe ser ~0)\n",
            "    - Desv. Est.: 1.000339 (debe ser ~1)\n",
            "    - Min: -1.3202\n",
            "    - Max: 5.8305\n",
            "  FCVC:\n",
            "    - Media: -0.000000 (debe ser ~0)\n",
            "    - Desv. Est.: 1.000339 (debe ser ~1)\n",
            "    - Min: -2.7065\n",
            "    - Max: 1.0833\n",
            "  NCP:\n",
            "    - Media: -0.000000 (debe ser ~0)\n",
            "    - Desv. Est.: 1.000339 (debe ser ~1)\n",
            "    - Min: -2.1733\n",
            "    - Max: 1.6959\n"
          ]
        }
      ],
      "source": [
        "# Ajustar scaler SOLO con datos de train\n",
        "print(f\"\\n Ajustando scaler con datos de train...\")\n",
        "scaler.fit(X_train[numeric_cols_to_scale])\n",
        "\n",
        "# Aplicar transformación\n",
        "X_train_scaled = X_train.copy()\n",
        "X_test_scaled = X_test.copy()\n",
        "\n",
        "X_train_scaled[numeric_cols_to_scale] = scaler.transform(X_train[numeric_cols_to_scale])\n",
        "X_test_scaled[numeric_cols_to_scale] = scaler.transform(X_test[numeric_cols_to_scale])\n",
        "\n",
        "print(f\" Estandarización aplicada\")\n",
        "\n",
        "# Mostrar estadísticas después de estandarizar\n",
        "print(f\"\\n Estadísticas DESPUÉS de estandarizar (primeras 3 variables):\")\n",
        "for col in numeric_cols_to_scale[:3]:\n",
        "    print(f\"  {col}:\")\n",
        "    print(f\"    - Media: {X_train_scaled[col].mean():.6f} (debe ser ~0)\")\n",
        "    print(f\"    - Desv. Est.: {X_train_scaled[col].std():.6f} (debe ser ~1)\")\n",
        "    print(f\"    - Min: {X_train_scaled[col].min():.4f}\")\n",
        "    print(f\"    - Max: {X_train_scaled[col].max():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Encoding de Variable Objetivo (Ordinal)\n",
        "\n",
        "### ¿Qué hace este paso?\n",
        "\n",
        "Convierte las clases de texto a números ordinales manteniendo el orden.\n",
        "\n",
        "### ¿Por qué encoding ordinal?\n",
        "\n",
        "El problema es **regresión ordinal**: las clases tienen un orden natural:\n",
        "1. Insufficient_Weight (menor peso)\n",
        "2. Normal_Weight\n",
        "3. Overweight_Level_I\n",
        "4. Overweight_Level_II\n",
        "5. Obesity_Type_I\n",
        "6. Obesity_Type_II\n",
        "7. Obesity_Type_III (mayor peso)\n",
        "\n",
        "### Mapeo Ordinal\n",
        "\n",
        "```\n",
        "0: Insufficient_Weight\n",
        "1: Normal_Weight\n",
        "2: Overweight_Level_I\n",
        "3: Overweight_Level_II\n",
        "4: Obesity_Type_I\n",
        "5: Obesity_Type_II\n",
        "6: Obesity_Type_III\n",
        "```\n",
        "\n",
        "### ¿Por qué es importante mantener el orden?\n",
        "\n",
        "- Algunos modelos pueden aprovechar el orden (regresión ordinal)\n",
        "- Las métricas ordinales consideran la distancia en la escala\n",
        "- Clasificar Obesity_Type_I como Obesity_Type_III es peor que clasificarlo como Normal_Weight\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Variable objetivo codificada:\n",
            "  Mapeo de clases:\n",
            "    0: Insufficient_Weight\n",
            "    1: Normal_Weight\n",
            "    2: Overweight_Level_I\n",
            "    3: Overweight_Level_II\n",
            "    4: Obesity_Type_I\n",
            "    5: Obesity_Type_II\n",
            "    6: Obesity_Type_III\n",
            "\n",
            "  Distribución en Train (codificada):\n",
            "0    190\n",
            "1    201\n",
            "2    245\n",
            "3    208\n",
            "4    227\n",
            "5    203\n",
            "6    203\n",
            "Name: count, dtype: int64\n",
            "\n",
            "  Distribución en Test (codificada):\n",
            "0     82\n",
            "1     86\n",
            "2    106\n",
            "3     89\n",
            "4     97\n",
            "5     87\n",
            "6     87\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Definir orden ordinal\n",
        "ordinal_order = [\n",
        "    'Insufficient_Weight',\n",
        "    'Normal_Weight',\n",
        "    'Overweight_Level_I',\n",
        "    'Overweight_Level_II',\n",
        "    'Obesity_Type_I',\n",
        "    'Obesity_Type_II',\n",
        "    'Obesity_Type_III'\n",
        "]\n",
        "\n",
        "# Crear encoder ordinal\n",
        "target_encoder = LabelEncoder()\n",
        "target_encoder.fit(ordinal_order)\n",
        "\n",
        "# Aplicar encoding\n",
        "y_train_encoded = pd.Series(target_encoder.transform(y_train), index=y_train.index)\n",
        "y_test_encoded = pd.Series(target_encoder.transform(y_test), index=y_test.index)\n",
        "\n",
        "print(f\"\\n Variable objetivo codificada:\")\n",
        "print(f\"  Mapeo de clases:\")\n",
        "for i, clase in enumerate(ordinal_order):\n",
        "    print(f\"    {i}: {clase}\")\n",
        "\n",
        "print(f\"\\n  Distribución en Train (codificada):\")\n",
        "print(y_train_encoded.value_counts().sort_index())\n",
        "\n",
        "print(f\"\\n  Distribución en Test (codificada):\")\n",
        "print(y_test_encoded.value_counts().sort_index())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Guardado de Datos Preprocesados y Transformadores\n",
        "\n",
        "### ¿Qué hace este paso?\n",
        "\n",
        "Guarda los datos preprocesados y los transformadores para uso futuro.\n",
        "\n",
        "### ¿Por qué guardar los datos preprocesados?\n",
        "\n",
        "- **Reproducibilidad**: Podemos cargar los datos ya procesados sin ejecutar todo el notebook\n",
        "- **Eficiencia**: No necesitamos preprocesar cada vez\n",
        "- **Consistencia**: Aseguramos usar exactamente los mismos datos\n",
        "\n",
        "### ¿Por qué guardar los transformadores?\n",
        "\n",
        "**CRÍTICO**: Cuando tengamos nuevos datos para predecir:\n",
        "1. Debemos aplicar las **mismas transformaciones**\n",
        "2. El scaler debe usar los mismos parámetros (media y desviación estándar del train)\n",
        "3. Los encoders deben usar el mismo mapeo\n",
        "\n",
        "**Sin esto**: Los nuevos datos estarían en una escala diferente y las predicciones serían incorrectas.\n",
        "\n",
        "### Archivos que Guardamos\n",
        "\n",
        "- **Datos preprocesados**: X_train.csv, X_test.csv, y_train.csv, y_test.csv\n",
        "- **Transformadores**: scaler.pkl, target_encoder.pkl, label_encoders.pkl\n",
        "- **Información**: preprocessing_info.pkl (metadatos sobre el preprocesamiento)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Guardando datos preprocesados...\n",
            "   Datos guardados en 'data/processed/'\n"
          ]
        }
      ],
      "source": [
        "# Crear directorio si no existe\n",
        "os.makedirs('data/processed', exist_ok=True)\n",
        "os.makedirs('models/preprocessing', exist_ok=True)\n",
        "\n",
        "# Guardar datos preprocesados\n",
        "print(f\"\\n Guardando datos preprocesados...\")\n",
        "\n",
        "# Guardar como CSV (para inspección)\n",
        "X_train_scaled.to_csv('data/processed/X_train.csv', index=False)\n",
        "X_test_scaled.to_csv('data/processed/X_test.csv', index=False)\n",
        "y_train_encoded.to_csv('data/processed/y_train.csv', index=False)\n",
        "y_test_encoded.to_csv('data/processed/y_test.csv', index=False)\n",
        "\n",
        "# Guardar también las versiones originales (sin codificar) para referencia\n",
        "y_train.to_csv('data/processed/y_train_original.csv', index=False)\n",
        "y_test.to_csv('data/processed/y_test_original.csv', index=False)\n",
        "\n",
        "print(f\"   Datos guardados en 'data/processed/'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Guardando transformadores...\n",
            "   Transformadores guardados en 'models/preprocessing/'\n"
          ]
        }
      ],
      "source": [
        "# Guardar transformadores\n",
        "print(f\"\\n Guardando transformadores...\")\n",
        "\n",
        "with open('models/preprocessing/scaler.pkl', 'wb') as f:\n",
        "    pickle.dump(scaler, f)\n",
        "\n",
        "with open('models/preprocessing/target_encoder.pkl', 'wb') as f:\n",
        "    pickle.dump(target_encoder, f)\n",
        "\n",
        "with open('models/preprocessing/label_encoders.pkl', 'wb') as f:\n",
        "    pickle.dump(label_encoders, f)\n",
        "\n",
        "# Guardar información sobre las columnas\n",
        "preprocessing_info = {\n",
        "    'numeric_cols': numeric_cols,\n",
        "    'binary_cols': binary_cols,\n",
        "    'multi_cat_cols': multi_cat_cols,\n",
        "    'numeric_cols_to_scale': numeric_cols_to_scale,\n",
        "    'ordinal_order': ordinal_order,\n",
        "    'n_features': X_train_scaled.shape[1]\n",
        "}\n",
        "\n",
        "with open('models/preprocessing/preprocessing_info.pkl', 'wb') as f:\n",
        "    pickle.dump(preprocessing_info, f)\n",
        "\n",
        "print(f\"   Transformadores guardados en 'models/preprocessing/'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Resumen Final\n",
        "\n",
        "###  Lo que hemos completado:\n",
        "\n",
        "1. **Carga y exploración** del dataset original\n",
        "2. **Identificación** de tipos de variables (numéricas vs categóricas)\n",
        "3. **Encoding** de variables categóricas (Label + One-Hot)\n",
        "4. **Estandarización** de variables numéricas\n",
        "5. **División estratificada** 70-30 (Train/Test)\n",
        "6. **Encoding ordinal** de variable objetivo\n",
        "7. **Guardado** de datos preprocesados y transformadores\n",
        "\n",
        "###  Resultados:\n",
        "\n",
        "- **Dataset original**: 2111 registros, 17 columnas\n",
        "- **Características finales**: 26 variables (después de encoding)\n",
        "- **Train**: 1477 registros (70%)\n",
        "- **Test**: 634 registros (30%)\n",
        "\n",
        "###  Próximos Pasos:\n",
        "\n",
        "1. Los datos están listos para entrenar modelos\n",
        "2. Usar validación cruzada en el conjunto de train\n",
        "3. Evaluar en el conjunto de test al final\n",
        "\n",
        "### ️ Recordatorios Importantes:\n",
        "\n",
        "- **Nunca** usar el conjunto de test durante el entrenamiento\n",
        "- **Siempre** aplicar las mismas transformaciones a nuevos datos\n",
        "- **Guardar** los transformadores para producción\n",
        "\n",
        "---\n",
        "\n",
        "**Nota**: Este preprocesamiento es fundamental. Cualquier error aquí afectará todos los modelos que entrenemos después.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
