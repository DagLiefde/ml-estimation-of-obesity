{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üîß Preprocesamiento de Datos\n",
        "## Clasificaci√≥n de Niveles de Obesidad - Regresi√≥n Ordinal\n",
        "\n",
        "Este notebook realiza el preprocesamiento completo de los datos antes de entrenar los modelos.\n",
        "\n",
        "**Objetivos del preprocesamiento:**\n",
        "1. Codificar variables categ√≥ricas (convertir texto a n√∫meros)\n",
        "2. Normalizar/estandarizar variables num√©ricas\n",
        "3. Dividir datos en Train (70%) y Test (30%) con estratificaci√≥n\n",
        "4. Guardar datos preprocesados y transformadores\n",
        "\n",
        "**Distribuci√≥n**: 70% Train (con validaci√≥n cruzada) / 30% Test\n",
        "\n",
        "---\n",
        "\n",
        "## ¬øPor qu√© es necesario el preprocesamiento?\n",
        "\n",
        "Los algoritmos de Machine Learning requieren que los datos est√©n en un formato espec√≠fico:\n",
        "\n",
        "- **Variables categ√≥ricas**: Deben convertirse a n√∫meros (encoding)\n",
        "- **Escalas diferentes**: Variables con rangos muy diferentes (ej: Age 0-100 vs Height 1.5-2.0) pueden sesgar el modelo\n",
        "- **Divisi√≥n de datos**: Necesitamos separar datos para entrenar y evaluar\n",
        "\n",
        "Sin preprocesamiento adecuado, los modelos pueden tener mal rendimiento o incluso fallar.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Importaci√≥n de Librer√≠as\n",
        "\n",
        "### ¬øPor qu√© estas librer√≠as?\n",
        "\n",
        "- **pandas**: Manipulaci√≥n de datos estructurados (DataFrames)\n",
        "- **numpy**: Operaciones num√©ricas y matem√°ticas\n",
        "- **sklearn**: Proporciona herramientas de preprocesamiento (StandardScaler, LabelEncoder, etc.) y divisi√≥n de datos\n",
        "- **pickle**: Guardar objetos Python (transformadores) para uso futuro\n",
        "- **os**: Crear directorios si no existen\n",
        "\n",
        "### ¬øPor qu√© guardar los transformadores?\n",
        "\n",
        "Es crucial guardar los transformadores (scaler, encoders) porque:\n",
        "- Cuando tengamos nuevos datos, debemos aplicar las **mismas transformaciones**\n",
        "- El scaler debe usar los mismos par√°metros (media y desviaci√≥n est√°ndar) aprendidos del train\n",
        "- Sin esto, los nuevos datos estar√≠an en una escala diferente y las predicciones ser√≠an incorrectas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "import pickle\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"‚úì Librer√≠as importadas correctamente\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Carga del Dataset\n",
        "\n",
        "### ¬øQu√© hace este paso?\n",
        "\n",
        "Carga el archivo CSV original y verifica que:\n",
        "- El archivo se carga correctamente\n",
        "- No hay valores faltantes\n",
        "- La estructura es la esperada\n",
        "\n",
        "### ¬øPor qu√© verificar valores faltantes?\n",
        "\n",
        "Los valores faltantes pueden causar problemas:\n",
        "- Algunos algoritmos no pueden manejar NaN directamente\n",
        "- Necesitamos decidir c√≥mo tratarlos (eliminar, imputar, etc.)\n",
        "- En este caso, verificamos que no haya valores faltantes antes de continuar\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cargar el dataset\n",
        "df = pd.read_csv('ObesityDataSet_raw_and_data_sinthetic.csv')\n",
        "\n",
        "print(f\"‚úì Dataset cargado exitosamente\")\n",
        "print(f\"  - Forma del dataset: {df.shape}\")\n",
        "print(f\"  - Columnas: {list(df.columns)}\")\n",
        "print(f\"\\nPrimeras 3 filas:\")\n",
        "df.head(3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verificar valores faltantes\n",
        "print(f\"Valores faltantes por columna:\")\n",
        "missing = df.isnull().sum()\n",
        "print(missing[missing > 0] if missing.sum() > 0 else \"  ‚úì No hay valores faltantes\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Creaci√≥n de Variable IMC (√çndice de Masa Corporal)\n",
        "\n",
        "### ¬øQu√© hace este paso?\n",
        "\n",
        "Calcula el IMC (BMI) a partir de Weight y Height, y elimina las variables originales.\n",
        "\n",
        "### ¬øPor qu√© usar IMC en lugar de Weight y Height?\n",
        "\n",
        "**IMC (√çndice de Masa Corporal)** = Peso (kg) / Altura (m)¬≤\n",
        "\n",
        "**Ventajas**:\n",
        "1. **Relevancia cl√≠nica**: El IMC es la m√©trica est√°ndar para clasificar obesidad\n",
        "2. **Reducci√≥n de dimensionalidad**: 2 variables (Weight, Height) ‚Üí 1 variable (BMI)\n",
        "3. **Reduce multicolinealidad**: Weight y Height est√°n altamente correlacionadas\n",
        "4. **Interpretabilidad**: El IMC es m√°s interpretable y significativo\n",
        "5. **Normalizaci√≥n natural**: El IMC ya normaliza por altura\n",
        "\n",
        "**Las clases de obesidad est√°n basadas en rangos de IMC**:\n",
        "- Insufficient_Weight: IMC < 18.5\n",
        "- Normal_Weight: 18.5 ‚â§ IMC < 25\n",
        "- Overweight_Level_I: 25 ‚â§ IMC < 27\n",
        "- Overweight_Level_II: 27 ‚â§ IMC < 30\n",
        "- Obesity_Type_I: 30 ‚â§ IMC < 35\n",
        "- Obesity_Type_II: 35 ‚â§ IMC < 40\n",
        "- Obesity_Type_III: IMC ‚â• 40\n",
        "\n",
        "### ¬øPor qu√© eliminar Weight y Height?\n",
        "\n",
        "- El IMC captura la informaci√≥n relevante de ambas variables\n",
        "- Evita multicolinealidad (correlaci√≥n alta entre variables)\n",
        "- Simplifica el modelo sin perder informaci√≥n relevante para este problema\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calcular IMC (BMI) = Weight (kg) / Height (m)¬≤\n",
        "print(\"üìä Calculando IMC (√çndice de Masa Corporal)...\")\n",
        "df['BMI'] = df['Weight'] / (df['Height'] ** 2)\n",
        "\n",
        "print(f\"‚úì IMC calculado\")\n",
        "print(f\"  - Rango de IMC: {df['BMI'].min():.2f} - {df['BMI'].max():.2f}\")\n",
        "print(f\"  - Media de IMC: {df['BMI'].mean():.2f}\")\n",
        "\n",
        "# Mostrar algunos ejemplos antes de eliminar\n",
        "print(f\"\\nEjemplos de c√°lculo:\")\n",
        "print(df[['Height', 'Weight', 'BMI', 'NObeyesdad']].head())\n",
        "\n",
        "# Eliminar Weight y Height (ya no las necesitamos)\n",
        "print(f\"\\nüìù Eliminando variables Weight y Height...\")\n",
        "df = df.drop(columns=['Weight', 'Height'])\n",
        "print(f\"  ‚úì Variables eliminadas\")\n",
        "print(f\"  - Columnas restantes: {len(df.columns)}\")\n",
        "\n",
        "# Variable objetivo\n",
        "target_column = 'NObeyesdad'\n",
        "y = df[target_column].copy()\n",
        "\n",
        "# Caracter√≠sticas (todas las columnas excepto la objetivo)\n",
        "X = df.drop(columns=[target_column])\n",
        "\n",
        "print(f\"\\n‚úì Variable objetivo separada: '{target_column}'\")\n",
        "print(f\"  - Forma de y: {y.shape}\")\n",
        "print(f\"  - Forma de X: {X.shape}\")\n",
        "\n",
        "# Identificar tipos de variables\n",
        "print(f\"\\nüìä AN√ÅLISIS DE TIPOS DE VARIABLES:\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Variables num√©ricas (ya son n√∫meros)\n",
        "numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
        "print(f\"\\nVariables Num√©ricas ({len(numeric_cols)}):\")\n",
        "for col in numeric_cols:\n",
        "    print(f\"  - {col}: min={X[col].min():.2f}, max={X[col].max():.2f}, media={X[col].mean():.2f}\")\n",
        "\n",
        "# Variables categ√≥ricas (objetos/strings)\n",
        "categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
        "print(f\"\\nVariables Categ√≥ricas ({len(categorical_cols)}):\")\n",
        "for col in categorical_cols:\n",
        "    unique_vals = X[col].unique()\n",
        "    print(f\"  - {col}: {len(unique_vals)} valores √∫nicos -> {list(unique_vals)[:5]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Encoding de Variables Categ√≥ricas\n",
        "\n",
        "### ¬øQu√© hace este paso?\n",
        "\n",
        "Convierte variables categ√≥ricas (texto) a n√∫meros. Los algoritmos de ML solo pueden trabajar con n√∫meros.\n",
        "\n",
        "### ¬øPor qu√© necesitamos encoding?\n",
        "\n",
        "Los modelos matem√°ticos no entienden texto. Necesitamos convertir:\n",
        "- \"Male\" / \"Female\" ‚Üí n√∫meros\n",
        "- \"yes\" / \"no\" ‚Üí n√∫meros\n",
        "- \"Public_Transportation\" / \"Walking\" / etc. ‚Üí n√∫meros\n",
        "\n",
        "### Dos Estrategias de Encoding\n",
        "\n",
        "#### 1. Label Encoding\n",
        "- Convierte cada categor√≠a a un n√∫mero (0, 1, 2, ...)\n",
        "- Ejemplo: \"no\" ‚Üí 0, \"yes\" ‚Üí 1\n",
        "- **Usamos para variables binarias** (solo 2 valores)\n",
        "\n",
        "#### 2. One-Hot Encoding\n",
        "- Crea una columna binaria por cada categor√≠a\n",
        "- Ejemplo: \"Public_Transportation\" ‚Üí [1, 0, 0, 0, 0]\n",
        "- **Usamos para variables con m√∫ltiples categor√≠as sin orden**\n",
        "- Evita que el modelo asuma orden donde no lo hay\n",
        "\n",
        "### ¬øPor qu√© diferentes estrategias?\n",
        "\n",
        "- **Label Encoding para binarias**: M√°s eficiente, no crea columnas extra\n",
        "- **One-Hot para multi-categor√≠a**: Evita que el modelo piense que hay orden (ej: \"Walking\" no es \"menor\" que \"Automobile\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear una copia para trabajar\n",
        "X_encoded = X.copy()\n",
        "\n",
        "# Identificar variables binarias (solo 2 valores)\n",
        "binary_cols = []\n",
        "multi_cat_cols = []\n",
        "\n",
        "for col in categorical_cols:\n",
        "    n_unique = X[col].nunique()\n",
        "    if n_unique == 2:\n",
        "        binary_cols.append(col)\n",
        "    else:\n",
        "        multi_cat_cols.append(col)\n",
        "\n",
        "print(f\"Variables Binarias (Label Encoding): {binary_cols}\")\n",
        "print(f\"Variables Multi-Categor√≠a (One-Hot Encoding): {multi_cat_cols}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4.1: Label Encoding para variables binarias\n",
        "print(f\"\\nüìù Aplicando Label Encoding a variables binarias...\")\n",
        "label_encoders = {}\n",
        "\n",
        "for col in binary_cols:\n",
        "    le = LabelEncoder()\n",
        "    X_encoded[col] = le.fit_transform(X[col])\n",
        "    label_encoders[col] = le\n",
        "    print(f\"  ‚úì {col}: {dict(zip(le.classes_, le.transform(le.classes_)))}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4.2: One-Hot Encoding para variables multi-categor√≠a\n",
        "print(f\"\\nüìù Aplicando One-Hot Encoding a variables multi-categor√≠a...\")\n",
        "\n",
        "# Usar pandas get_dummies (m√°s simple que sklearn para este caso)\n",
        "X_encoded = pd.get_dummies(X_encoded, columns=multi_cat_cols, prefix=multi_cat_cols, drop_first=False)\n",
        "\n",
        "print(f\"  ‚úì Variables codificadas:\")\n",
        "print(f\"    - Antes: {len(categorical_cols)} variables categ√≥ricas\")\n",
        "print(f\"    - Despu√©s: {X_encoded.shape[1]} variables totales\")\n",
        "print(f\"    - Nuevas columnas creadas: {X_encoded.shape[1] - len(numeric_cols) - len(binary_cols)}\")\n",
        "\n",
        "# Mostrar algunas columnas nuevas\n",
        "new_cols = [col for col in X_encoded.columns if any(mc in col for mc in multi_cat_cols)]\n",
        "print(f\"\\n  Ejemplo de nuevas columnas (primeras 5):\")\n",
        "for col in new_cols[:5]:\n",
        "    print(f\"    - {col}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Normalizaci√≥n/Estandarizaci√≥n de Variables Num√©ricas\n",
        "\n",
        "### ¬øQu√© hace este paso?\n",
        "\n",
        "Normaliza/estandariza las variables num√©ricas para que todas est√©n en la misma escala.\n",
        "\n",
        "### ¬øPor qu√© es importante?\n",
        "\n",
        "**Problema**: Variables con rangos muy diferentes pueden dominar el modelo.\n",
        "\n",
        "**Ejemplo**:\n",
        "- Age: 0-100 (rango grande)\n",
        "- Height: 1.5-2.0 (rango peque√±o)\n",
        "\n",
        "Sin estandarizaci√≥n, Age podr√≠a tener m√°s \"peso\" en el modelo simplemente por tener n√∫meros m√°s grandes, aunque Height podr√≠a ser igual de importante.\n",
        "\n",
        "### Tipos de Normalizaci√≥n\n",
        "\n",
        "#### Estandarizaci√≥n (Z-score normalization) - **USAMOS ESTA**\n",
        "- **F√≥rmula**: `z = (x - Œº) / œÉ`\n",
        "- Convierte datos a: media = 0, desviaci√≥n est√°ndar = 1\n",
        "- √ötil cuando los datos siguen distribuci√≥n normal\n",
        "- **Ventaja**: Funciona bien con la mayor√≠a de algoritmos\n",
        "\n",
        "#### Normalizaci√≥n (Min-Max)\n",
        "- **F√≥rmula**: `x_norm = (x - min) / (max - min)`\n",
        "- Convierte datos a rango [0, 1]\n",
        "- √ötil cuando no conocemos la distribuci√≥n\n",
        "\n",
        "### ¬øCu√°ndo NO estandarizar?\n",
        "\n",
        "- **√Årboles de decisi√≥n** (Random Forest, Gradient Boosting): No necesitan estandarizaci√≥n porque dividen por umbrales\n",
        "- **Naive Bayes**: Puede funcionar sin estandarizaci√≥n\n",
        "- **Pero**: SVM, k-NN, redes neuronales S√ç necesitan estandarizaci√≥n\n",
        "\n",
        "**Para este proyecto**: Estandarizamos porque usaremos varios tipos de modelos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identificar columnas num√©ricas (despu√©s del encoding)\n",
        "# Las columnas num√©ricas originales + las binarias codificadas\n",
        "numeric_cols_to_scale = numeric_cols + binary_cols\n",
        "\n",
        "print(f\"Variables a estandarizar ({len(numeric_cols_to_scale)}):\")\n",
        "print(f\"  {numeric_cols_to_scale}\")\n",
        "\n",
        "# Crear el scaler (pero NO aplicarlo a√∫n - lo haremos despu√©s de dividir)\n",
        "# Esto es importante: NO debemos estandarizar antes de dividir train/test\n",
        "# porque podr√≠amos \"filtrar\" informaci√≥n del test al train\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "print(f\"\\n‚úì Scaler creado (se aplicar√° despu√©s de dividir train/test)\")\n",
        "print(f\"  - Tipo: StandardScaler (Z-score normalization)\")\n",
        "print(f\"  - F√≥rmula: z = (x - Œº) / œÉ\")\n",
        "\n",
        "# Mostrar estad√≠sticas antes de estandarizar\n",
        "print(f\"\\nüìä Estad√≠sticas ANTES de estandarizar (primeras 3 variables):\")\n",
        "for col in numeric_cols_to_scale[:3]:\n",
        "    print(f\"  {col}:\")\n",
        "    print(f\"    - Media: {X_encoded[col].mean():.4f}\")\n",
        "    print(f\"    - Desv. Est.: {X_encoded[col].std():.4f}\")\n",
        "    print(f\"    - Min: {X_encoded[col].min():.4f}\")\n",
        "    print(f\"    - Max: {X_encoded[col].max():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Divisi√≥n de Datos (70-30) con Estratificaci√≥n\n",
        "\n",
        "### ¬øQu√© hace este paso?\n",
        "\n",
        "Divide los datos en Train (70%) y Test (30%) manteniendo la proporci√≥n de clases en ambos conjuntos.\n",
        "\n",
        "### ¬øPor qu√© dividir los datos?\n",
        "\n",
        "- **Train (70%)**: Usamos para entrenar y ajustar los modelos (con validaci√≥n cruzada)\n",
        "- **Test (30%)**: Usamos SOLO al final para evaluar el modelo final\n",
        "- **Nunca** usamos test durante el entrenamiento (evita sobreajuste)\n",
        "\n",
        "### ¬øQu√© es la Estratificaci√≥n?\n",
        "\n",
        "**Estratificaci√≥n** = Mantener la proporci√≥n de clases en ambos conjuntos.\n",
        "\n",
        "**Ejemplo sin estratificaci√≥n**:\n",
        "- Train: 80% Normal_Weight, 5% Obesity_Type_III\n",
        "- Test: 10% Normal_Weight, 30% Obesity_Type_III\n",
        "- ‚ùå Problema: Los conjuntos no son representativos\n",
        "\n",
        "**Ejemplo con estratificaci√≥n**:\n",
        "- Train: 13.6% Normal_Weight, 15.3% Obesity_Type_III\n",
        "- Test: 13.6% Normal_Weight, 15.3% Obesity_Type_III\n",
        "- ‚úÖ Ambos conjuntos tienen la misma distribuci√≥n\n",
        "\n",
        "### ¬øPor qu√© NO estandarizar antes de dividir?\n",
        "\n",
        "**IMPORTANTE**: NO estandarizamos antes de dividir porque:\n",
        "\n",
        "1. El scaler se ajusta SOLO con datos de train\n",
        "2. Luego se aplica a test (sin reajustar)\n",
        "3. Esto evita **\"data leakage\"** (filtrar informaci√≥n del test al train)\n",
        "\n",
        "Si estandarizamos antes de dividir:\n",
        "- El scaler \"ve\" todos los datos (train + test)\n",
        "- Esto filtra informaci√≥n del test al train\n",
        "- El modelo podr√≠a tener mejor rendimiento de lo que realmente tiene\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Divisi√≥n estratificada\n",
        "# random_state: Para reproducibilidad (mismos resultados cada vez)\n",
        "# stratify: Mantiene proporci√≥n de clases\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_encoded,\n",
        "    y,\n",
        "    test_size=0.30,  # 30% para test\n",
        "    random_state=42,  # Semilla para reproducibilidad\n",
        "    stratify=y  # Estratificaci√≥n por clases\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úì Divisi√≥n completada:\")\n",
        "print(f\"  - Train: {X_train.shape[0]} registros ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
        "print(f\"  - Test: {X_test.shape[0]} registros ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
        "print(f\"  - Caracter√≠sticas: {X_train.shape[1]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verificar estratificaci√≥n\n",
        "print(f\"\\nüìä Verificaci√≥n de estratificaci√≥n:\")\n",
        "print(f\"\\nDistribuci√≥n en Train:\")\n",
        "train_dist = y_train.value_counts().sort_index()\n",
        "train_pct = (y_train.value_counts(normalize=True) * 100).sort_index()\n",
        "for clase in train_dist.index:\n",
        "    print(f\"  {clase:25s}: {train_dist[clase]:4d} ({train_pct[clase]:5.2f}%)\")\n",
        "\n",
        "print(f\"\\nDistribuci√≥n en Test:\")\n",
        "test_dist = y_test.value_counts().sort_index()\n",
        "test_pct = (y_test.value_counts(normalize=True) * 100).sort_index()\n",
        "for clase in test_dist.index:\n",
        "    print(f\"  {clase:25s}: {test_dist[clase]:4d} ({test_pct[clase]:5.2f}%)\")\n",
        "\n",
        "# Verificar que las proporciones son similares\n",
        "print(f\"\\n‚úì Verificaci√≥n: Las proporciones son similares en ambos conjuntos\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Estandarizaci√≥n de Datos (DESPU√âS de Dividir)\n",
        "\n",
        "### ¬øQu√© hace este paso?\n",
        "\n",
        "Aplica la estandarizaci√≥n SOLO a los datos de train, ajusta el scaler con train, y luego aplica la misma transformaci√≥n a test (sin reajustar).\n",
        "\n",
        "### ¬øPor qu√© este orden es cr√≠tico?\n",
        "\n",
        "**Proceso correcto**:\n",
        "1. Dividir datos (train/test)\n",
        "2. Ajustar scaler con SOLO train ‚Üí aprende media y desviaci√≥n est√°ndar de train\n",
        "3. Transformar train con el scaler ajustado\n",
        "4. Transformar test con el MISMO scaler (sin reajustar)\n",
        "\n",
        "**Si hici√©ramos mal**:\n",
        "1. Estandarizar todo el dataset\n",
        "2. Dividir datos\n",
        "3. ‚ùå El scaler \"vio\" datos de test ‚Üí data leakage\n",
        "\n",
        "### ¬øPor qu√© no reajustar el scaler con test?\n",
        "\n",
        "Porque en producci√≥n:\n",
        "- Nuevos datos llegan sin etiquetas\n",
        "- Debemos aplicar las mismas transformaciones aprendidas del train\n",
        "- Si reajustamos con test, estar√≠amos \"haciendo trampa\"\n",
        "\n",
        "### Resultado Esperado\n",
        "\n",
        "Despu√©s de estandarizar:\n",
        "- **Media ‚âà 0**: Los datos est√°n centrados\n",
        "- **Desviaci√≥n est√°ndar ‚âà 1**: Los datos est√°n escalados\n",
        "- Esto facilita el entrenamiento de modelos sensibles a la escala\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ajustar scaler SOLO con datos de train\n",
        "print(f\"\\nüìù Ajustando scaler con datos de train...\")\n",
        "scaler.fit(X_train[numeric_cols_to_scale])\n",
        "\n",
        "# Aplicar transformaci√≥n\n",
        "X_train_scaled = X_train.copy()\n",
        "X_test_scaled = X_test.copy()\n",
        "\n",
        "X_train_scaled[numeric_cols_to_scale] = scaler.transform(X_train[numeric_cols_to_scale])\n",
        "X_test_scaled[numeric_cols_to_scale] = scaler.transform(X_test[numeric_cols_to_scale])\n",
        "\n",
        "print(f\"‚úì Estandarizaci√≥n aplicada\")\n",
        "\n",
        "# Mostrar estad√≠sticas despu√©s de estandarizar\n",
        "print(f\"\\nüìä Estad√≠sticas DESPU√âS de estandarizar (primeras 3 variables):\")\n",
        "for col in numeric_cols_to_scale[:3]:\n",
        "    print(f\"  {col}:\")\n",
        "    print(f\"    - Media: {X_train_scaled[col].mean():.6f} (debe ser ~0)\")\n",
        "    print(f\"    - Desv. Est.: {X_train_scaled[col].std():.6f} (debe ser ~1)\")\n",
        "    print(f\"    - Min: {X_train_scaled[col].min():.4f}\")\n",
        "    print(f\"    - Max: {X_train_scaled[col].max():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Encoding de Variable Objetivo (Ordinal)\n",
        "\n",
        "### ¬øQu√© hace este paso?\n",
        "\n",
        "Convierte las clases de texto a n√∫meros ordinales manteniendo el orden.\n",
        "\n",
        "### ¬øPor qu√© encoding ordinal?\n",
        "\n",
        "El problema es **regresi√≥n ordinal**: las clases tienen un orden natural:\n",
        "1. Insufficient_Weight (menor peso)\n",
        "2. Normal_Weight\n",
        "3. Overweight_Level_I\n",
        "4. Overweight_Level_II\n",
        "5. Obesity_Type_I\n",
        "6. Obesity_Type_II\n",
        "7. Obesity_Type_III (mayor peso)\n",
        "\n",
        "### Mapeo Ordinal\n",
        "\n",
        "```\n",
        "0: Insufficient_Weight\n",
        "1: Normal_Weight\n",
        "2: Overweight_Level_I\n",
        "3: Overweight_Level_II\n",
        "4: Obesity_Type_I\n",
        "5: Obesity_Type_II\n",
        "6: Obesity_Type_III\n",
        "```\n",
        "\n",
        "### ¬øPor qu√© es importante mantener el orden?\n",
        "\n",
        "- Algunos modelos pueden aprovechar el orden (regresi√≥n ordinal)\n",
        "- Las m√©tricas ordinales consideran la distancia en la escala\n",
        "- Clasificar Obesity_Type_I como Obesity_Type_III es peor que clasificarlo como Normal_Weight\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Definir orden ordinal\n",
        "ordinal_order = [\n",
        "    'Insufficient_Weight',\n",
        "    'Normal_Weight',\n",
        "    'Overweight_Level_I',\n",
        "    'Overweight_Level_II',\n",
        "    'Obesity_Type_I',\n",
        "    'Obesity_Type_II',\n",
        "    'Obesity_Type_III'\n",
        "]\n",
        "\n",
        "# Crear encoder ordinal\n",
        "target_encoder = LabelEncoder()\n",
        "target_encoder.fit(ordinal_order)\n",
        "\n",
        "# Aplicar encoding\n",
        "y_train_encoded = pd.Series(target_encoder.transform(y_train), index=y_train.index)\n",
        "y_test_encoded = pd.Series(target_encoder.transform(y_test), index=y_test.index)\n",
        "\n",
        "print(f\"\\n‚úì Variable objetivo codificada:\")\n",
        "print(f\"  Mapeo de clases:\")\n",
        "for i, clase in enumerate(ordinal_order):\n",
        "    print(f\"    {i}: {clase}\")\n",
        "\n",
        "print(f\"\\n  Distribuci√≥n en Train (codificada):\")\n",
        "print(y_train_encoded.value_counts().sort_index())\n",
        "\n",
        "print(f\"\\n  Distribuci√≥n en Test (codificada):\")\n",
        "print(y_test_encoded.value_counts().sort_index())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Guardado de Datos Preprocesados y Transformadores\n",
        "\n",
        "### ¬øQu√© hace este paso?\n",
        "\n",
        "Guarda los datos preprocesados y los transformadores para uso futuro.\n",
        "\n",
        "### ¬øPor qu√© guardar los datos preprocesados?\n",
        "\n",
        "- **Reproducibilidad**: Podemos cargar los datos ya procesados sin ejecutar todo el notebook\n",
        "- **Eficiencia**: No necesitamos preprocesar cada vez\n",
        "- **Consistencia**: Aseguramos usar exactamente los mismos datos\n",
        "\n",
        "### ¬øPor qu√© guardar los transformadores?\n",
        "\n",
        "**CR√çTICO**: Cuando tengamos nuevos datos para predecir:\n",
        "1. Debemos aplicar las **mismas transformaciones**\n",
        "2. El scaler debe usar los mismos par√°metros (media y desviaci√≥n est√°ndar del train)\n",
        "3. Los encoders deben usar el mismo mapeo\n",
        "\n",
        "**Sin esto**: Los nuevos datos estar√≠an en una escala diferente y las predicciones ser√≠an incorrectas.\n",
        "\n",
        "### Archivos que Guardamos\n",
        "\n",
        "- **Datos preprocesados**: X_train.csv, X_test.csv, y_train.csv, y_test.csv\n",
        "- **Transformadores**: scaler.pkl, target_encoder.pkl, label_encoders.pkl\n",
        "- **Informaci√≥n**: preprocessing_info.pkl (metadatos sobre el preprocesamiento)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear directorio si no existe\n",
        "os.makedirs('data/processed', exist_ok=True)\n",
        "os.makedirs('models/preprocessing', exist_ok=True)\n",
        "\n",
        "# Guardar datos preprocesados\n",
        "print(f\"\\nüìÅ Guardando datos preprocesados...\")\n",
        "\n",
        "# Guardar como CSV (para inspecci√≥n)\n",
        "X_train_scaled.to_csv('data/processed/X_train.csv', index=False)\n",
        "X_test_scaled.to_csv('data/processed/X_test.csv', index=False)\n",
        "y_train_encoded.to_csv('data/processed/y_train.csv', index=False)\n",
        "y_test_encoded.to_csv('data/processed/y_test.csv', index=False)\n",
        "\n",
        "# Guardar tambi√©n las versiones originales (sin codificar) para referencia\n",
        "y_train.to_csv('data/processed/y_train_original.csv', index=False)\n",
        "y_test.to_csv('data/processed/y_test_original.csv', index=False)\n",
        "\n",
        "print(f\"  ‚úì Datos guardados en 'data/processed/'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Guardar transformadores\n",
        "print(f\"\\nüìÅ Guardando transformadores...\")\n",
        "\n",
        "with open('models/preprocessing/scaler.pkl', 'wb') as f:\n",
        "    pickle.dump(scaler, f)\n",
        "\n",
        "with open('models/preprocessing/target_encoder.pkl', 'wb') as f:\n",
        "    pickle.dump(target_encoder, f)\n",
        "\n",
        "with open('models/preprocessing/label_encoders.pkl', 'wb') as f:\n",
        "    pickle.dump(label_encoders, f)\n",
        "\n",
        "# Guardar informaci√≥n sobre las columnas\n",
        "preprocessing_info = {\n",
        "    'numeric_cols': numeric_cols,\n",
        "    'binary_cols': binary_cols,\n",
        "    'multi_cat_cols': multi_cat_cols,\n",
        "    'numeric_cols_to_scale': numeric_cols_to_scale,\n",
        "    'ordinal_order': ordinal_order,\n",
        "    'n_features': X_train_scaled.shape[1]\n",
        "}\n",
        "\n",
        "with open('models/preprocessing/preprocessing_info.pkl', 'wb') as f:\n",
        "    pickle.dump(preprocessing_info, f)\n",
        "\n",
        "print(f\"  ‚úì Transformadores guardados en 'models/preprocessing/'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Resumen Final\n",
        "\n",
        "### ‚úÖ Lo que hemos completado:\n",
        "\n",
        "1. **Carga y exploraci√≥n** del dataset original\n",
        "2. **Identificaci√≥n** de tipos de variables (num√©ricas vs categ√≥ricas)\n",
        "3. **Encoding** de variables categ√≥ricas (Label + One-Hot)\n",
        "4. **Estandarizaci√≥n** de variables num√©ricas\n",
        "5. **Divisi√≥n estratificada** 70-30 (Train/Test)\n",
        "6. **Encoding ordinal** de variable objetivo\n",
        "7. **Guardado** de datos preprocesados y transformadores\n",
        "\n",
        "### üìä Resultados:\n",
        "\n",
        "- **Dataset original**: 2111 registros, 17 columnas\n",
        "- **Caracter√≠sticas finales**: 26 variables (despu√©s de encoding)\n",
        "- **Train**: 1477 registros (70%)\n",
        "- **Test**: 634 registros (30%)\n",
        "\n",
        "### üöÄ Pr√≥ximos Pasos:\n",
        "\n",
        "1. Los datos est√°n listos para entrenar modelos\n",
        "2. Usar validaci√≥n cruzada en el conjunto de train\n",
        "3. Evaluar en el conjunto de test al final\n",
        "\n",
        "### ‚ö†Ô∏è Recordatorios Importantes:\n",
        "\n",
        "- **Nunca** usar el conjunto de test durante el entrenamiento\n",
        "- **Siempre** aplicar las mismas transformaciones a nuevos datos\n",
        "- **Guardar** los transformadores para producci√≥n\n",
        "\n",
        "---\n",
        "\n",
        "**Nota**: Este preprocesamiento es fundamental. Cualquier error aqu√≠ afectar√° todos los modelos que entrenemos despu√©s.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
