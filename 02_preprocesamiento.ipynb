{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üîß Preprocesamiento de Datos\n",
        "## Clasificaci√≥n de Niveles de Obesidad - Regresi√≥n Ordinal\n",
        "\n",
        "Este notebook realiza el preprocesamiento completo de los datos antes de entrenar los modelos.\n",
        "\n",
        "**Objetivos del preprocesamiento:**\n",
        "1. Codificar variables categ√≥ricas (convertir texto a n√∫meros)\n",
        "2. Normalizar/estandarizar variables num√©ricas\n",
        "3. Dividir datos en Train (70%) y Test (30%) con estratificaci√≥n\n",
        "4. Guardar datos preprocesados y transformadores\n",
        "\n",
        "**Distribuci√≥n**: 70% Train (con validaci√≥n cruzada) / 30% Test\n",
        "\n",
        "---\n",
        "\n",
        "## ¬øPor qu√© es necesario el preprocesamiento?\n",
        "\n",
        "Los algoritmos de Machine Learning requieren que los datos est√©n en un formato espec√≠fico:\n",
        "\n",
        "- **Variables categ√≥ricas**: Deben convertirse a n√∫meros (encoding)\n",
        "- **Escalas diferentes**: Variables con rangos muy diferentes (ej: Age 0-100 vs Height 1.5-2.0) pueden sesgar el modelo\n",
        "- **Divisi√≥n de datos**: Necesitamos separar datos para entrenar y evaluar\n",
        "\n",
        "Sin preprocesamiento adecuado, los modelos pueden tener mal rendimiento o incluso fallar.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Importaci√≥n de Librer√≠as\n",
        "\n",
        "### ¬øPor qu√© estas librer√≠as?\n",
        "\n",
        "- **pandas**: Manipulaci√≥n de datos estructurados (DataFrames)\n",
        "- **numpy**: Operaciones num√©ricas y matem√°ticas\n",
        "- **sklearn**: Proporciona herramientas de preprocesamiento (StandardScaler, LabelEncoder, etc.) y divisi√≥n de datos\n",
        "- **pickle**: Guardar objetos Python (transformadores) para uso futuro\n",
        "- **os**: Crear directorios si no existen\n",
        "\n",
        "### ¬øPor qu√© guardar los transformadores?\n",
        "\n",
        "Es crucial guardar los transformadores (scaler, encoders) porque:\n",
        "- Cuando tengamos nuevos datos, debemos aplicar las **mismas transformaciones**\n",
        "- El scaler debe usar los mismos par√°metros (media y desviaci√≥n est√°ndar) aprendidos del train\n",
        "- Sin esto, los nuevos datos estar√≠an en una escala diferente y las predicciones ser√≠an incorrectas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Librer√≠as importadas correctamente\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "import pickle\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"‚úì Librer√≠as importadas correctamente\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Carga del Dataset\n",
        "\n",
        "### ¬øQu√© hace este paso?\n",
        "\n",
        "Carga el archivo CSV original y verifica que:\n",
        "- El archivo se carga correctamente\n",
        "- No hay valores faltantes\n",
        "- La estructura es la esperada\n",
        "\n",
        "### ¬øPor qu√© verificar valores faltantes?\n",
        "\n",
        "Los valores faltantes pueden causar problemas:\n",
        "- Algunos algoritmos no pueden manejar NaN directamente\n",
        "- Necesitamos decidir c√≥mo tratarlos (eliminar, imputar, etc.)\n",
        "- En este caso, verificamos que no haya valores faltantes antes de continuar\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Dataset cargado exitosamente\n",
            "  - Forma del dataset: (2111, 17)\n",
            "  - Columnas: ['Gender', 'Age', 'Height', 'Weight', 'family_history_with_overweight', 'FAVC', 'FCVC', 'NCP', 'CAEC', 'SMOKE', 'CH2O', 'SCC', 'FAF', 'TUE', 'CALC', 'MTRANS', 'NObeyesdad']\n",
            "\n",
            "Primeras 3 filas:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Gender</th>\n",
              "      <th>Age</th>\n",
              "      <th>Height</th>\n",
              "      <th>Weight</th>\n",
              "      <th>family_history_with_overweight</th>\n",
              "      <th>FAVC</th>\n",
              "      <th>FCVC</th>\n",
              "      <th>NCP</th>\n",
              "      <th>CAEC</th>\n",
              "      <th>SMOKE</th>\n",
              "      <th>CH2O</th>\n",
              "      <th>SCC</th>\n",
              "      <th>FAF</th>\n",
              "      <th>TUE</th>\n",
              "      <th>CALC</th>\n",
              "      <th>MTRANS</th>\n",
              "      <th>NObeyesdad</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Female</td>\n",
              "      <td>21.0</td>\n",
              "      <td>1.62</td>\n",
              "      <td>64.0</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>Sometimes</td>\n",
              "      <td>no</td>\n",
              "      <td>2.0</td>\n",
              "      <td>no</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>no</td>\n",
              "      <td>Public_Transportation</td>\n",
              "      <td>Normal_Weight</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Female</td>\n",
              "      <td>21.0</td>\n",
              "      <td>1.52</td>\n",
              "      <td>56.0</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>Sometimes</td>\n",
              "      <td>yes</td>\n",
              "      <td>3.0</td>\n",
              "      <td>yes</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Sometimes</td>\n",
              "      <td>Public_Transportation</td>\n",
              "      <td>Normal_Weight</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Male</td>\n",
              "      <td>23.0</td>\n",
              "      <td>1.80</td>\n",
              "      <td>77.0</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>Sometimes</td>\n",
              "      <td>no</td>\n",
              "      <td>2.0</td>\n",
              "      <td>no</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Frequently</td>\n",
              "      <td>Public_Transportation</td>\n",
              "      <td>Normal_Weight</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Gender   Age  Height  Weight family_history_with_overweight FAVC  FCVC  \\\n",
              "0  Female  21.0    1.62    64.0                            yes   no   2.0   \n",
              "1  Female  21.0    1.52    56.0                            yes   no   3.0   \n",
              "2    Male  23.0    1.80    77.0                            yes   no   2.0   \n",
              "\n",
              "   NCP       CAEC SMOKE  CH2O  SCC  FAF  TUE        CALC  \\\n",
              "0  3.0  Sometimes    no   2.0   no  0.0  1.0          no   \n",
              "1  3.0  Sometimes   yes   3.0  yes  3.0  0.0   Sometimes   \n",
              "2  3.0  Sometimes    no   2.0   no  2.0  1.0  Frequently   \n",
              "\n",
              "                  MTRANS     NObeyesdad  \n",
              "0  Public_Transportation  Normal_Weight  \n",
              "1  Public_Transportation  Normal_Weight  \n",
              "2  Public_Transportation  Normal_Weight  "
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Cargar el dataset\n",
        "df = pd.read_csv('ObesityDataSet_raw_and_data_sinthetic.csv')\n",
        "\n",
        "print(f\"‚úì Dataset cargado exitosamente\")\n",
        "print(f\"  - Forma del dataset: {df.shape}\")\n",
        "print(f\"  - Columnas: {list(df.columns)}\")\n",
        "print(f\"\\nPrimeras 3 filas:\")\n",
        "df.head(3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Valores faltantes por columna:\n",
            "  ‚úì No hay valores faltantes\n"
          ]
        }
      ],
      "source": [
        "# Verificar valores faltantes\n",
        "print(f\"Valores faltantes por columna:\")\n",
        "missing = df.isnull().sum()\n",
        "print(missing[missing > 0] if missing.sum() > 0 else \"  ‚úì No hay valores faltantes\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Creaci√≥n de Variable IMC (√çndice de Masa Corporal)\n",
        "\n",
        "### ¬øQu√© hace este paso?\n",
        "\n",
        "Calcula el IMC (BMI) a partir de Weight y Height, y elimina las variables originales.\n",
        "\n",
        "### ¬øPor qu√© usar IMC en lugar de Weight y Height?\n",
        "\n",
        "**IMC (√çndice de Masa Corporal)** = Peso (kg) / Altura (m)¬≤\n",
        "\n",
        "**Ventajas**:\n",
        "1. **Relevancia cl√≠nica**: El IMC es la m√©trica est√°ndar para clasificar obesidad\n",
        "2. **Reducci√≥n de dimensionalidad**: 2 variables (Weight, Height) ‚Üí 1 variable (BMI)\n",
        "3. **Reduce multicolinealidad**: Weight y Height est√°n altamente correlacionadas\n",
        "4. **Interpretabilidad**: El IMC es m√°s interpretable y significativo\n",
        "5. **Normalizaci√≥n natural**: El IMC ya normaliza por altura\n",
        "\n",
        "**Las clases de obesidad est√°n basadas en rangos de IMC**:\n",
        "- Insufficient_Weight: IMC < 18.5\n",
        "- Normal_Weight: 18.5 ‚â§ IMC < 25\n",
        "- Overweight_Level_I: 25 ‚â§ IMC < 27\n",
        "- Overweight_Level_II: 27 ‚â§ IMC < 30\n",
        "- Obesity_Type_I: 30 ‚â§ IMC < 35\n",
        "- Obesity_Type_II: 35 ‚â§ IMC < 40\n",
        "- Obesity_Type_III: IMC ‚â• 40\n",
        "\n",
        "### ¬øPor qu√© eliminar Weight y Height?\n",
        "\n",
        "- El IMC captura la informaci√≥n relevante de ambas variables\n",
        "- Evita multicolinealidad (correlaci√≥n alta entre variables)\n",
        "- Simplifica el modelo sin perder informaci√≥n relevante para este problema\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Calculando IMC (√çndice de Masa Corporal)...\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "'Weight'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "File \u001b[1;32mc:\\Users\\GARC√çA\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
            "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
            "\u001b[1;31mKeyError\u001b[0m: 'Weight'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[5], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Calcular IMC (BMI) = Weight (kg) / Height (m)¬≤\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müìä Calculando IMC (√çndice de Masa Corporal)...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBMI\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mWeight\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m/\u001b[39m (df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHeight\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úì IMC calculado\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  - Rango de IMC: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBMI\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmin()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBMI\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\GARC√çA\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
            "File \u001b[1;32mc:\\Users\\GARC√çA\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
            "\u001b[1;31mKeyError\u001b[0m: 'Weight'"
          ]
        }
      ],
      "source": [
        "# Calcular IMC (BMI) = Weight (kg) / Height (m)¬≤\n",
        "print(\"üìä Calculando IMC (√çndice de Masa Corporal)...\")\n",
        "df['BMI'] = df['Weight'] / (df['Height'] ** 2)\n",
        "\n",
        "print(f\"‚úì IMC calculado\")\n",
        "print(f\"  - Rango de IMC: {df['BMI'].min():.2f} - {df['BMI'].max():.2f}\")\n",
        "print(f\"  - Media de IMC: {df['BMI'].mean():.2f}\")\n",
        "\n",
        "# Mostrar algunos ejemplos antes de eliminar\n",
        "print(f\"\\nEjemplos de c√°lculo:\")\n",
        "print(df[['Height', 'Weight', 'BMI', 'NObeyesdad']].head())\n",
        "\n",
        "# Eliminar Weight y Height (ya no las necesitamos)\n",
        "print(f\"\\nüìù Eliminando variables Weight y Height...\")\n",
        "df = df.drop(columns=['Weight', 'Height'])\n",
        "print(f\"  ‚úì Variables eliminadas\")\n",
        "print(f\"  - Columnas restantes: {len(df.columns)}\")\n",
        "\n",
        "# Variable objetivo\n",
        "target_column = 'NObeyesdad'\n",
        "y = df[target_column].copy()\n",
        "\n",
        "# Caracter√≠sticas (todas las columnas excepto la objetivo)\n",
        "X = df.drop(columns=[target_column])\n",
        "\n",
        "print(f\"\\n‚úì Variable objetivo separada: '{target_column}'\")\n",
        "print(f\"  - Forma de y: {y.shape}\")\n",
        "print(f\"  - Forma de X: {X.shape}\")\n",
        "\n",
        "# Identificar tipos de variables\n",
        "print(f\"\\nüìä AN√ÅLISIS DE TIPOS DE VARIABLES:\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Variables num√©ricas (ya son n√∫meros)\n",
        "numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
        "print(f\"\\nVariables Num√©ricas ({len(numeric_cols)}):\")\n",
        "for col in numeric_cols:\n",
        "    print(f\"  - {col}: min={X[col].min():.2f}, max={X[col].max():.2f}, media={X[col].mean():.2f}\")\n",
        "\n",
        "# Variables categ√≥ricas (objetos/strings)\n",
        "categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
        "print(f\"\\nVariables Categ√≥ricas ({len(categorical_cols)}):\")\n",
        "for col in categorical_cols:\n",
        "    unique_vals = X[col].unique()\n",
        "    print(f\"  - {col}: {len(unique_vals)} valores √∫nicos -> {list(unique_vals)[:5]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Encoding de Variables Categ√≥ricas\n",
        "\n",
        "### ¬øQu√© hace este paso?\n",
        "\n",
        "Convierte variables categ√≥ricas (texto) a n√∫meros. Los algoritmos de ML solo pueden trabajar con n√∫meros.\n",
        "\n",
        "### ¬øPor qu√© necesitamos encoding?\n",
        "\n",
        "Los modelos matem√°ticos no entienden texto. Necesitamos convertir:\n",
        "- \"Male\" / \"Female\" ‚Üí n√∫meros\n",
        "- \"yes\" / \"no\" ‚Üí n√∫meros\n",
        "- \"Public_Transportation\" / \"Walking\" / etc. ‚Üí n√∫meros\n",
        "\n",
        "### Dos Estrategias de Encoding\n",
        "\n",
        "#### 1. Label Encoding\n",
        "- Convierte cada categor√≠a a un n√∫mero (0, 1, 2, ...)\n",
        "- Ejemplo: \"no\" ‚Üí 0, \"yes\" ‚Üí 1\n",
        "- **Usamos para variables binarias** (solo 2 valores)\n",
        "\n",
        "#### 2. One-Hot Encoding\n",
        "- Crea una columna binaria por cada categor√≠a\n",
        "- Ejemplo: \"Public_Transportation\" ‚Üí [1, 0, 0, 0, 0]\n",
        "- **Usamos para variables con m√∫ltiples categor√≠as sin orden**\n",
        "- Evita que el modelo asuma orden donde no lo hay\n",
        "\n",
        "### ¬øPor qu√© diferentes estrategias?\n",
        "\n",
        "- **Label Encoding para binarias**: M√°s eficiente, no crea columnas extra\n",
        "- **One-Hot para multi-categor√≠a**: Evita que el modelo piense que hay orden (ej: \"Walking\" no es \"menor\" que \"Automobile\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Variables Binarias (Label Encoding): ['Gender', 'family_history_with_overweight', 'FAVC', 'SMOKE', 'SCC']\n",
            "Variables Multi-Categor√≠a (One-Hot Encoding): ['CAEC', 'CALC', 'MTRANS']\n"
          ]
        }
      ],
      "source": [
        "# Crear una copia para trabajar\n",
        "X_encoded = X.copy()\n",
        "\n",
        "# Identificar variables binarias (solo 2 valores)\n",
        "binary_cols = []\n",
        "multi_cat_cols = []\n",
        "\n",
        "for col in categorical_cols:\n",
        "    n_unique = X[col].nunique()\n",
        "    if n_unique == 2:\n",
        "        binary_cols.append(col)\n",
        "    else:\n",
        "        multi_cat_cols.append(col)\n",
        "\n",
        "print(f\"Variables Binarias (Label Encoding): {binary_cols}\")\n",
        "print(f\"Variables Multi-Categor√≠a (One-Hot Encoding): {multi_cat_cols}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìù Aplicando Label Encoding a variables binarias...\n",
            "  ‚úì Gender: {'Female': 0, 'Male': 1}\n",
            "  ‚úì family_history_with_overweight: {'no': 0, 'yes': 1}\n",
            "  ‚úì FAVC: {'no': 0, 'yes': 1}\n",
            "  ‚úì SMOKE: {'no': 0, 'yes': 1}\n",
            "  ‚úì SCC: {'no': 0, 'yes': 1}\n"
          ]
        }
      ],
      "source": [
        "# 4.1: Label Encoding para variables binarias\n",
        "print(f\"\\nüìù Aplicando Label Encoding a variables binarias...\")\n",
        "label_encoders = {}\n",
        "\n",
        "for col in binary_cols:\n",
        "    le = LabelEncoder()\n",
        "    X_encoded[col] = le.fit_transform(X[col])\n",
        "    label_encoders[col] = le\n",
        "    print(f\"  ‚úì {col}: {dict(zip(le.classes_, le.transform(le.classes_)))}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìù Aplicando One-Hot Encoding a variables multi-categor√≠a...\n",
            "  ‚úì Variables codificadas:\n",
            "    - Antes: 8 variables categ√≥ricas\n",
            "    - Despu√©s: 25 variables totales\n",
            "    - Nuevas columnas creadas: 13\n",
            "\n",
            "  Ejemplo de nuevas columnas (primeras 5):\n",
            "    - CAEC_Always\n",
            "    - CAEC_Frequently\n",
            "    - CAEC_Sometimes\n",
            "    - CAEC_no\n",
            "    - CALC_Always\n"
          ]
        }
      ],
      "source": [
        "# 4.2: One-Hot Encoding para variables multi-categor√≠a\n",
        "print(f\"\\nüìù Aplicando One-Hot Encoding a variables multi-categor√≠a...\")\n",
        "\n",
        "# Usar pandas get_dummies (m√°s simple que sklearn para este caso)\n",
        "X_encoded = pd.get_dummies(X_encoded, columns=multi_cat_cols, prefix=multi_cat_cols, drop_first=False)\n",
        "\n",
        "print(f\"  ‚úì Variables codificadas:\")\n",
        "print(f\"    - Antes: {len(categorical_cols)} variables categ√≥ricas\")\n",
        "print(f\"    - Despu√©s: {X_encoded.shape[1]} variables totales\")\n",
        "print(f\"    - Nuevas columnas creadas: {X_encoded.shape[1] - len(numeric_cols) - len(binary_cols)}\")\n",
        "\n",
        "# Mostrar algunas columnas nuevas\n",
        "new_cols = [col for col in X_encoded.columns if any(mc in col for mc in multi_cat_cols)]\n",
        "print(f\"\\n  Ejemplo de nuevas columnas (primeras 5):\")\n",
        "for col in new_cols[:5]:\n",
        "    print(f\"    - {col}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Normalizaci√≥n/Estandarizaci√≥n de Variables Num√©ricas\n",
        "\n",
        "### ¬øQu√© hace este paso?\n",
        "\n",
        "Normaliza/estandariza las variables num√©ricas para que todas est√©n en la misma escala.\n",
        "\n",
        "### ¬øPor qu√© es importante?\n",
        "\n",
        "**Problema**: Variables con rangos muy diferentes pueden dominar el modelo.\n",
        "\n",
        "**Ejemplo**:\n",
        "- Age: 0-100 (rango grande)\n",
        "- Height: 1.5-2.0 (rango peque√±o)\n",
        "\n",
        "Sin estandarizaci√≥n, Age podr√≠a tener m√°s \"peso\" en el modelo simplemente por tener n√∫meros m√°s grandes, aunque Height podr√≠a ser igual de importante.\n",
        "\n",
        "### Tipos de Normalizaci√≥n\n",
        "\n",
        "#### Estandarizaci√≥n (Z-score normalization) - **USAMOS ESTA**\n",
        "- **F√≥rmula**: `z = (x - Œº) / œÉ`\n",
        "- Convierte datos a: media = 0, desviaci√≥n est√°ndar = 1\n",
        "- √ötil cuando los datos siguen distribuci√≥n normal\n",
        "- **Ventaja**: Funciona bien con la mayor√≠a de algoritmos\n",
        "\n",
        "#### Normalizaci√≥n (Min-Max)\n",
        "- **F√≥rmula**: `x_norm = (x - min) / (max - min)`\n",
        "- Convierte datos a rango [0, 1]\n",
        "- √ötil cuando no conocemos la distribuci√≥n\n",
        "\n",
        "### ¬øCu√°ndo NO estandarizar?\n",
        "\n",
        "- **√Årboles de decisi√≥n** (Random Forest, Gradient Boosting): No necesitan estandarizaci√≥n porque dividen por umbrales\n",
        "- **Naive Bayes**: Puede funcionar sin estandarizaci√≥n\n",
        "- **Pero**: SVM, k-NN, redes neuronales S√ç necesitan estandarizaci√≥n\n",
        "\n",
        "**Para este proyecto**: Estandarizamos porque usaremos varios tipos de modelos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Variables a estandarizar (12):\n",
            "  ['Age', 'FCVC', 'NCP', 'CH2O', 'FAF', 'TUE', 'BMI', 'Gender', 'family_history_with_overweight', 'FAVC', 'SMOKE', 'SCC']\n",
            "\n",
            "‚úì Scaler creado (se aplicar√° despu√©s de dividir train/test)\n",
            "  - Tipo: StandardScaler (Z-score normalization)\n",
            "  - F√≥rmula: z = (x - Œº) / œÉ\n",
            "\n",
            "üìä Estad√≠sticas ANTES de estandarizar (primeras 3 variables):\n",
            "  Age:\n",
            "    - Media: 24.3126\n",
            "    - Desv. Est.: 6.3460\n",
            "    - Min: 14.0000\n",
            "    - Max: 61.0000\n",
            "  FCVC:\n",
            "    - Media: 2.4190\n",
            "    - Desv. Est.: 0.5339\n",
            "    - Min: 1.0000\n",
            "    - Max: 3.0000\n",
            "  NCP:\n",
            "    - Media: 2.6856\n",
            "    - Desv. Est.: 0.7780\n",
            "    - Min: 1.0000\n",
            "    - Max: 4.0000\n"
          ]
        }
      ],
      "source": [
        "# Identificar columnas num√©ricas (despu√©s del encoding)\n",
        "# Las columnas num√©ricas originales + las binarias codificadas\n",
        "numeric_cols_to_scale = numeric_cols + binary_cols\n",
        "\n",
        "print(f\"Variables a estandarizar ({len(numeric_cols_to_scale)}):\")\n",
        "print(f\"  {numeric_cols_to_scale}\")\n",
        "\n",
        "# Crear el scaler (pero NO aplicarlo a√∫n - lo haremos despu√©s de dividir)\n",
        "# Esto es importante: NO debemos estandarizar antes de dividir train/test\n",
        "# porque podr√≠amos \"filtrar\" informaci√≥n del test al train\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "print(f\"\\n‚úì Scaler creado (se aplicar√° despu√©s de dividir train/test)\")\n",
        "print(f\"  - Tipo: StandardScaler (Z-score normalization)\")\n",
        "print(f\"  - F√≥rmula: z = (x - Œº) / œÉ\")\n",
        "\n",
        "# Mostrar estad√≠sticas antes de estandarizar\n",
        "print(f\"\\nüìä Estad√≠sticas ANTES de estandarizar (primeras 3 variables):\")\n",
        "for col in numeric_cols_to_scale[:3]:\n",
        "    print(f\"  {col}:\")\n",
        "    print(f\"    - Media: {X_encoded[col].mean():.4f}\")\n",
        "    print(f\"    - Desv. Est.: {X_encoded[col].std():.4f}\")\n",
        "    print(f\"    - Min: {X_encoded[col].min():.4f}\")\n",
        "    print(f\"    - Max: {X_encoded[col].max():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Divisi√≥n de Datos (70-30) con Estratificaci√≥n\n",
        "\n",
        "### ¬øQu√© hace este paso?\n",
        "\n",
        "Divide los datos en Train (70%) y Test (30%) manteniendo la proporci√≥n de clases en ambos conjuntos.\n",
        "\n",
        "### ¬øPor qu√© dividir los datos?\n",
        "\n",
        "- **Train (70%)**: Usamos para entrenar y ajustar los modelos (con validaci√≥n cruzada)\n",
        "- **Test (30%)**: Usamos SOLO al final para evaluar el modelo final\n",
        "- **Nunca** usamos test durante el entrenamiento (evita sobreajuste)\n",
        "\n",
        "### ¬øQu√© es la Estratificaci√≥n?\n",
        "\n",
        "**Estratificaci√≥n** = Mantener la proporci√≥n de clases en ambos conjuntos.\n",
        "\n",
        "**Ejemplo sin estratificaci√≥n**:\n",
        "- Train: 80% Normal_Weight, 5% Obesity_Type_III\n",
        "- Test: 10% Normal_Weight, 30% Obesity_Type_III\n",
        "- ‚ùå Problema: Los conjuntos no son representativos\n",
        "\n",
        "**Ejemplo con estratificaci√≥n**:\n",
        "- Train: 13.6% Normal_Weight, 15.3% Obesity_Type_III\n",
        "- Test: 13.6% Normal_Weight, 15.3% Obesity_Type_III\n",
        "- ‚úÖ Ambos conjuntos tienen la misma distribuci√≥n\n",
        "\n",
        "### ¬øPor qu√© NO estandarizar antes de dividir?\n",
        "\n",
        "**IMPORTANTE**: NO estandarizamos antes de dividir porque:\n",
        "\n",
        "1. El scaler se ajusta SOLO con datos de train\n",
        "2. Luego se aplica a test (sin reajustar)\n",
        "3. Esto evita **\"data leakage\"** (filtrar informaci√≥n del test al train)\n",
        "\n",
        "Si estandarizamos antes de dividir:\n",
        "- El scaler \"ve\" todos los datos (train + test)\n",
        "- Esto filtra informaci√≥n del test al train\n",
        "- El modelo podr√≠a tener mejor rendimiento de lo que realmente tiene\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úì Divisi√≥n completada:\n",
            "  - Train: 1477 registros (70.0%)\n",
            "  - Test: 634 registros (30.0%)\n",
            "  - Caracter√≠sticas: 25\n"
          ]
        }
      ],
      "source": [
        "# Divisi√≥n estratificada\n",
        "# random_state: Para reproducibilidad (mismos resultados cada vez)\n",
        "# stratify: Mantiene proporci√≥n de clases\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_encoded,\n",
        "    y,\n",
        "    test_size=0.30,  # 30% para test\n",
        "    random_state=42,  # Semilla para reproducibilidad\n",
        "    stratify=y  # Estratificaci√≥n por clases\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úì Divisi√≥n completada:\")\n",
        "print(f\"  - Train: {X_train.shape[0]} registros ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
        "print(f\"  - Test: {X_test.shape[0]} registros ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
        "print(f\"  - Caracter√≠sticas: {X_train.shape[1]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä Verificaci√≥n de estratificaci√≥n:\n",
            "\n",
            "Distribuci√≥n en Train:\n",
            "  Insufficient_Weight      :  190 (12.86%)\n",
            "  Normal_Weight            :  201 (13.61%)\n",
            "  Obesity_Type_I           :  245 (16.59%)\n",
            "  Obesity_Type_II          :  208 (14.08%)\n",
            "  Obesity_Type_III         :  227 (15.37%)\n",
            "  Overweight_Level_I       :  203 (13.74%)\n",
            "  Overweight_Level_II      :  203 (13.74%)\n",
            "\n",
            "Distribuci√≥n en Test:\n",
            "  Insufficient_Weight      :   82 (12.93%)\n",
            "  Normal_Weight            :   86 (13.56%)\n",
            "  Obesity_Type_I           :  106 (16.72%)\n",
            "  Obesity_Type_II          :   89 (14.04%)\n",
            "  Obesity_Type_III         :   97 (15.30%)\n",
            "  Overweight_Level_I       :   87 (13.72%)\n",
            "  Overweight_Level_II      :   87 (13.72%)\n",
            "\n",
            "‚úì Verificaci√≥n: Las proporciones son similares en ambos conjuntos\n"
          ]
        }
      ],
      "source": [
        "# Verificar estratificaci√≥n\n",
        "print(f\"\\nüìä Verificaci√≥n de estratificaci√≥n:\")\n",
        "print(f\"\\nDistribuci√≥n en Train:\")\n",
        "train_dist = y_train.value_counts().sort_index()\n",
        "train_pct = (y_train.value_counts(normalize=True) * 100).sort_index()\n",
        "for clase in train_dist.index:\n",
        "    print(f\"  {clase:25s}: {train_dist[clase]:4d} ({train_pct[clase]:5.2f}%)\")\n",
        "\n",
        "print(f\"\\nDistribuci√≥n en Test:\")\n",
        "test_dist = y_test.value_counts().sort_index()\n",
        "test_pct = (y_test.value_counts(normalize=True) * 100).sort_index()\n",
        "for clase in test_dist.index:\n",
        "    print(f\"  {clase:25s}: {test_dist[clase]:4d} ({test_pct[clase]:5.2f}%)\")\n",
        "\n",
        "# Verificar que las proporciones son similares\n",
        "print(f\"\\n‚úì Verificaci√≥n: Las proporciones son similares en ambos conjuntos\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Estandarizaci√≥n de Datos (DESPU√âS de Dividir)\n",
        "\n",
        "### ¬øQu√© hace este paso?\n",
        "\n",
        "Aplica la estandarizaci√≥n SOLO a los datos de train, ajusta el scaler con train, y luego aplica la misma transformaci√≥n a test (sin reajustar).\n",
        "\n",
        "### ¬øPor qu√© este orden es cr√≠tico?\n",
        "\n",
        "**Proceso correcto**:\n",
        "1. Dividir datos (train/test)\n",
        "2. Ajustar scaler con SOLO train ‚Üí aprende media y desviaci√≥n est√°ndar de train\n",
        "3. Transformar train con el scaler ajustado\n",
        "4. Transformar test con el MISMO scaler (sin reajustar)\n",
        "\n",
        "**Si hici√©ramos mal**:\n",
        "1. Estandarizar todo el dataset\n",
        "2. Dividir datos\n",
        "3. ‚ùå El scaler \"vio\" datos de test ‚Üí data leakage\n",
        "\n",
        "### ¬øPor qu√© no reajustar el scaler con test?\n",
        "\n",
        "Porque en producci√≥n:\n",
        "- Nuevos datos llegan sin etiquetas\n",
        "- Debemos aplicar las mismas transformaciones aprendidas del train\n",
        "- Si reajustamos con test, estar√≠amos \"haciendo trampa\"\n",
        "\n",
        "### Resultado Esperado\n",
        "\n",
        "Despu√©s de estandarizar:\n",
        "- **Media ‚âà 0**: Los datos est√°n centrados\n",
        "- **Desviaci√≥n est√°ndar ‚âà 1**: Los datos est√°n escalados\n",
        "- Esto facilita el entrenamiento de modelos sensibles a la escala\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìù Ajustando scaler con datos de train...\n",
            "‚úì Estandarizaci√≥n aplicada\n",
            "\n",
            "üìä Estad√≠sticas DESPU√âS de estandarizar (primeras 3 variables):\n",
            "  Age:\n",
            "    - Media: 0.000000 (debe ser ~0)\n",
            "    - Desv. Est.: 1.000339 (debe ser ~1)\n",
            "    - Min: -1.3202\n",
            "    - Max: 5.8305\n",
            "  FCVC:\n",
            "    - Media: -0.000000 (debe ser ~0)\n",
            "    - Desv. Est.: 1.000339 (debe ser ~1)\n",
            "    - Min: -2.7065\n",
            "    - Max: 1.0833\n",
            "  NCP:\n",
            "    - Media: -0.000000 (debe ser ~0)\n",
            "    - Desv. Est.: 1.000339 (debe ser ~1)\n",
            "    - Min: -2.1733\n",
            "    - Max: 1.6959\n"
          ]
        }
      ],
      "source": [
        "# Ajustar scaler SOLO con datos de train\n",
        "print(f\"\\nüìù Ajustando scaler con datos de train...\")\n",
        "scaler.fit(X_train[numeric_cols_to_scale])\n",
        "\n",
        "# Aplicar transformaci√≥n\n",
        "X_train_scaled = X_train.copy()\n",
        "X_test_scaled = X_test.copy()\n",
        "\n",
        "X_train_scaled[numeric_cols_to_scale] = scaler.transform(X_train[numeric_cols_to_scale])\n",
        "X_test_scaled[numeric_cols_to_scale] = scaler.transform(X_test[numeric_cols_to_scale])\n",
        "\n",
        "print(f\"‚úì Estandarizaci√≥n aplicada\")\n",
        "\n",
        "# Mostrar estad√≠sticas despu√©s de estandarizar\n",
        "print(f\"\\nüìä Estad√≠sticas DESPU√âS de estandarizar (primeras 3 variables):\")\n",
        "for col in numeric_cols_to_scale[:3]:\n",
        "    print(f\"  {col}:\")\n",
        "    print(f\"    - Media: {X_train_scaled[col].mean():.6f} (debe ser ~0)\")\n",
        "    print(f\"    - Desv. Est.: {X_train_scaled[col].std():.6f} (debe ser ~1)\")\n",
        "    print(f\"    - Min: {X_train_scaled[col].min():.4f}\")\n",
        "    print(f\"    - Max: {X_train_scaled[col].max():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Encoding de Variable Objetivo (Ordinal)\n",
        "\n",
        "### ¬øQu√© hace este paso?\n",
        "\n",
        "Convierte las clases de texto a n√∫meros ordinales manteniendo el orden.\n",
        "\n",
        "### ¬øPor qu√© encoding ordinal?\n",
        "\n",
        "El problema es **regresi√≥n ordinal**: las clases tienen un orden natural:\n",
        "1. Insufficient_Weight (menor peso)\n",
        "2. Normal_Weight\n",
        "3. Overweight_Level_I\n",
        "4. Overweight_Level_II\n",
        "5. Obesity_Type_I\n",
        "6. Obesity_Type_II\n",
        "7. Obesity_Type_III (mayor peso)\n",
        "\n",
        "### Mapeo Ordinal\n",
        "\n",
        "```\n",
        "0: Insufficient_Weight\n",
        "1: Normal_Weight\n",
        "2: Overweight_Level_I\n",
        "3: Overweight_Level_II\n",
        "4: Obesity_Type_I\n",
        "5: Obesity_Type_II\n",
        "6: Obesity_Type_III\n",
        "```\n",
        "\n",
        "### ¬øPor qu√© es importante mantener el orden?\n",
        "\n",
        "- Algunos modelos pueden aprovechar el orden (regresi√≥n ordinal)\n",
        "- Las m√©tricas ordinales consideran la distancia en la escala\n",
        "- Clasificar Obesity_Type_I como Obesity_Type_III es peor que clasificarlo como Normal_Weight\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úì Variable objetivo codificada:\n",
            "  Mapeo de clases:\n",
            "    0: Insufficient_Weight\n",
            "    1: Normal_Weight\n",
            "    2: Overweight_Level_I\n",
            "    3: Overweight_Level_II\n",
            "    4: Obesity_Type_I\n",
            "    5: Obesity_Type_II\n",
            "    6: Obesity_Type_III\n",
            "\n",
            "  Distribuci√≥n en Train (codificada):\n",
            "0    190\n",
            "1    201\n",
            "2    245\n",
            "3    208\n",
            "4    227\n",
            "5    203\n",
            "6    203\n",
            "Name: count, dtype: int64\n",
            "\n",
            "  Distribuci√≥n en Test (codificada):\n",
            "0     82\n",
            "1     86\n",
            "2    106\n",
            "3     89\n",
            "4     97\n",
            "5     87\n",
            "6     87\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Definir orden ordinal\n",
        "ordinal_order = [\n",
        "    'Insufficient_Weight',\n",
        "    'Normal_Weight',\n",
        "    'Overweight_Level_I',\n",
        "    'Overweight_Level_II',\n",
        "    'Obesity_Type_I',\n",
        "    'Obesity_Type_II',\n",
        "    'Obesity_Type_III'\n",
        "]\n",
        "\n",
        "# Crear encoder ordinal\n",
        "target_encoder = LabelEncoder()\n",
        "target_encoder.fit(ordinal_order)\n",
        "\n",
        "# Aplicar encoding\n",
        "y_train_encoded = pd.Series(target_encoder.transform(y_train), index=y_train.index)\n",
        "y_test_encoded = pd.Series(target_encoder.transform(y_test), index=y_test.index)\n",
        "\n",
        "print(f\"\\n‚úì Variable objetivo codificada:\")\n",
        "print(f\"  Mapeo de clases:\")\n",
        "for i, clase in enumerate(ordinal_order):\n",
        "    print(f\"    {i}: {clase}\")\n",
        "\n",
        "print(f\"\\n  Distribuci√≥n en Train (codificada):\")\n",
        "print(y_train_encoded.value_counts().sort_index())\n",
        "\n",
        "print(f\"\\n  Distribuci√≥n en Test (codificada):\")\n",
        "print(y_test_encoded.value_counts().sort_index())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Guardado de Datos Preprocesados y Transformadores\n",
        "\n",
        "### ¬øQu√© hace este paso?\n",
        "\n",
        "Guarda los datos preprocesados y los transformadores para uso futuro.\n",
        "\n",
        "### ¬øPor qu√© guardar los datos preprocesados?\n",
        "\n",
        "- **Reproducibilidad**: Podemos cargar los datos ya procesados sin ejecutar todo el notebook\n",
        "- **Eficiencia**: No necesitamos preprocesar cada vez\n",
        "- **Consistencia**: Aseguramos usar exactamente los mismos datos\n",
        "\n",
        "### ¬øPor qu√© guardar los transformadores?\n",
        "\n",
        "**CR√çTICO**: Cuando tengamos nuevos datos para predecir:\n",
        "1. Debemos aplicar las **mismas transformaciones**\n",
        "2. El scaler debe usar los mismos par√°metros (media y desviaci√≥n est√°ndar del train)\n",
        "3. Los encoders deben usar el mismo mapeo\n",
        "\n",
        "**Sin esto**: Los nuevos datos estar√≠an en una escala diferente y las predicciones ser√≠an incorrectas.\n",
        "\n",
        "### Archivos que Guardamos\n",
        "\n",
        "- **Datos preprocesados**: X_train.csv, X_test.csv, y_train.csv, y_test.csv\n",
        "- **Transformadores**: scaler.pkl, target_encoder.pkl, label_encoders.pkl\n",
        "- **Informaci√≥n**: preprocessing_info.pkl (metadatos sobre el preprocesamiento)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìÅ Guardando datos preprocesados...\n",
            "  ‚úì Datos guardados en 'data/processed/'\n"
          ]
        }
      ],
      "source": [
        "# Crear directorio si no existe\n",
        "os.makedirs('data/processed', exist_ok=True)\n",
        "os.makedirs('models/preprocessing', exist_ok=True)\n",
        "\n",
        "# Guardar datos preprocesados\n",
        "print(f\"\\nüìÅ Guardando datos preprocesados...\")\n",
        "\n",
        "# Guardar como CSV (para inspecci√≥n)\n",
        "X_train_scaled.to_csv('data/processed/X_train.csv', index=False)\n",
        "X_test_scaled.to_csv('data/processed/X_test.csv', index=False)\n",
        "y_train_encoded.to_csv('data/processed/y_train.csv', index=False)\n",
        "y_test_encoded.to_csv('data/processed/y_test.csv', index=False)\n",
        "\n",
        "# Guardar tambi√©n las versiones originales (sin codificar) para referencia\n",
        "y_train.to_csv('data/processed/y_train_original.csv', index=False)\n",
        "y_test.to_csv('data/processed/y_test_original.csv', index=False)\n",
        "\n",
        "print(f\"  ‚úì Datos guardados en 'data/processed/'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìÅ Guardando transformadores...\n",
            "  ‚úì Transformadores guardados en 'models/preprocessing/'\n"
          ]
        }
      ],
      "source": [
        "# Guardar transformadores\n",
        "print(f\"\\nüìÅ Guardando transformadores...\")\n",
        "\n",
        "with open('models/preprocessing/scaler.pkl', 'wb') as f:\n",
        "    pickle.dump(scaler, f)\n",
        "\n",
        "with open('models/preprocessing/target_encoder.pkl', 'wb') as f:\n",
        "    pickle.dump(target_encoder, f)\n",
        "\n",
        "with open('models/preprocessing/label_encoders.pkl', 'wb') as f:\n",
        "    pickle.dump(label_encoders, f)\n",
        "\n",
        "# Guardar informaci√≥n sobre las columnas\n",
        "preprocessing_info = {\n",
        "    'numeric_cols': numeric_cols,\n",
        "    'binary_cols': binary_cols,\n",
        "    'multi_cat_cols': multi_cat_cols,\n",
        "    'numeric_cols_to_scale': numeric_cols_to_scale,\n",
        "    'ordinal_order': ordinal_order,\n",
        "    'n_features': X_train_scaled.shape[1]\n",
        "}\n",
        "\n",
        "with open('models/preprocessing/preprocessing_info.pkl', 'wb') as f:\n",
        "    pickle.dump(preprocessing_info, f)\n",
        "\n",
        "print(f\"  ‚úì Transformadores guardados en 'models/preprocessing/'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Resumen Final\n",
        "\n",
        "### ‚úÖ Lo que hemos completado:\n",
        "\n",
        "1. **Carga y exploraci√≥n** del dataset original\n",
        "2. **Identificaci√≥n** de tipos de variables (num√©ricas vs categ√≥ricas)\n",
        "3. **Encoding** de variables categ√≥ricas (Label + One-Hot)\n",
        "4. **Estandarizaci√≥n** de variables num√©ricas\n",
        "5. **Divisi√≥n estratificada** 70-30 (Train/Test)\n",
        "6. **Encoding ordinal** de variable objetivo\n",
        "7. **Guardado** de datos preprocesados y transformadores\n",
        "\n",
        "### üìä Resultados:\n",
        "\n",
        "- **Dataset original**: 2111 registros, 17 columnas\n",
        "- **Caracter√≠sticas finales**: 26 variables (despu√©s de encoding)\n",
        "- **Train**: 1477 registros (70%)\n",
        "- **Test**: 634 registros (30%)\n",
        "\n",
        "### üöÄ Pr√≥ximos Pasos:\n",
        "\n",
        "1. Los datos est√°n listos para entrenar modelos\n",
        "2. Usar validaci√≥n cruzada en el conjunto de train\n",
        "3. Evaluar en el conjunto de test al final\n",
        "\n",
        "### ‚ö†Ô∏è Recordatorios Importantes:\n",
        "\n",
        "- **Nunca** usar el conjunto de test durante el entrenamiento\n",
        "- **Siempre** aplicar las mismas transformaciones a nuevos datos\n",
        "- **Guardar** los transformadores para producci√≥n\n",
        "\n",
        "---\n",
        "\n",
        "**Nota**: Este preprocesamiento es fundamental. Cualquier error aqu√≠ afectar√° todos los modelos que entrenemos despu√©s.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
